{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# California Housing Price Prediction con Amazon SageMaker Autopilot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El kernel `Python 3 (Data Science)` funciona bien con este notebook dentro de SageMaker Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Contenido\n",
    "\n",
    "1. [Introducción](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Preparación de los datos de entrenamiento](#Data)\n",
    "1. [Entrenamiento](#Settingup)\n",
    "1. [Lanzando el job de SageMaker Autopilot](#Results)\n",
    "1. [Evaluación utilizando los datos de test](#Evaluate)\n",
    "1. [Cleanup](#Cleanup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Introducción<a name=\"Introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker Autopilot es una solución de aprendizaje automático (comúnmente conocida como AutoML) para conjuntos de datos tabulares. Podemos utilizar SageMaker Autopilot de diferentes maneras:\n",
    "\n",
    "- en piloto automático (sin ninguna entrada humana) \n",
    "- con orientación humana\n",
    "- sin código a través de SageMaker Studio\n",
    "- con scripts utilizando los AWS SDK.\n",
    "\n",
    "Este cuaderno utilizará los SDKs de AWS para crear e implementar de forma sencilla un modelo de aprendizaje automático sin hacer feature engineering manualmente. También exploraremos el informe de importancia de features autogenerado.\n",
    "\n",
    "La predicción de los precios de la vivienda es un problema clásico de regresión lineal en ML. El Dataset describe a las viviendas que se encuentran en un determinado distrito de California (como grupo) e incluye estadísticas resumidas sobre ellas basadas en los datos del censo de 1990.\n",
    "\n",
    "Las variables del conjunto de datos son fácilmente comprensibles, y las columnas son las siguientes:\n",
    "\n",
    "* ```longitude```\n",
    "* ```latitude```\n",
    "* ```housingMedianAge```\n",
    "* ```totalRooms```\n",
    "* ```totalBedrooms```\n",
    "* ```population```\n",
    "* ```households```\n",
    "* ```medianIncome```\n",
    "* ```medianHouseValue``` (target)\n",
    "\n",
    "Lo que vamos a intentar predecir es el valor medio de la vivienda en un distrito. Dejaremos que Autopilot realice feature engineering, model selection, tuning de los mismos y nos dé el mejor modelo candidato listo para utilizarlo en las inferencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup\n",
    "\n",
    "_Este notebook fue creado y probado en una instancia `ml.t3.medium`._\n",
    "\n",
    "Empecemos por especificar:\n",
    "\n",
    "- El bucket de S3 y el prefijo a utilizar para los datos de entrenamiento y del modelo.  Este debería estar dentro de la misma región que la instancia de Notebook, el entrenamiento y el alojamiento. El siguiente código utilizará el bucket de S3 por defecto de SageMaker (o creará uno si no existe).\n",
    "- El ARN del rol IAM utilizado para dar acceso a los datos. El siguiente código utilizará el rol de ejecución de SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "isConfigCell": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "bucket = session.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-autopilot-housing\"\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, importaremos las bibliotecas de Python que necesitaremos para el resto del ejercicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from urllib.parse import urlparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Preparación de los datos de entrenamiento <a name=\"Data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos el Dataset de vivienda de California, de acceso público. La variable objetivo es el valor medio de la vivienda en los distritos de California.\n",
    "\n",
    "Este Dataset se obtuvo del repositorio StatLib (http://lib.stat.cmu.edu/datasets/) y se derivó del censo de EE.UU. de 1990, utilizando una fila por grupo de bloques del censo. Un grupo de bloques es la unidad geográfica más pequeña para la que la Oficina del Censo de EE.UU. publica datos de muestra (un grupo de bloques suele tener una población de entre 600 y 3.000 personas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos y extraemos el Dataset en el EFS montado en el Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-sample-files/datasets/tabular/california_housing/cal_housing.tgz to ./cal_housing.tgz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://sagemaker-sample-files/datasets/tabular/california_housing/cal_housing.tgz .\n",
    "!tar -zxf cal_housing.tgz -o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    \"housingMedianAge\",\n",
    "    \"totalRooms\",\n",
    "    \"totalBedrooms\",\n",
    "    \"population\",\n",
    "    \"households\",\n",
    "    \"medianIncome\",\n",
    "    \"medianHouseValue\",\n",
    "]\n",
    "\n",
    "target = \"medianHouseValue\"\n",
    "\n",
    "cal_housing_df = pd.read_csv(\"CaliforniaHousing/cal_housing.data\", names=columns, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de iniciar un trabajo de AutoML, es una buena práctica inspeccionar los datos para asegurarse de que no hay problemas obvios con ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housingMedianAge</th>\n",
       "      <th>totalRooms</th>\n",
       "      <th>totalBedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>medianIncome</th>\n",
       "      <th>medianHouseValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>-121.09</td>\n",
       "      <td>39.48</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1665.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>845.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>1.5603</td>\n",
       "      <td>78100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>-121.21</td>\n",
       "      <td>39.49</td>\n",
       "      <td>18.0</td>\n",
       "      <td>697.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>2.5568</td>\n",
       "      <td>77100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>-121.22</td>\n",
       "      <td>39.43</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2254.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>1.7000</td>\n",
       "      <td>92300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>-121.32</td>\n",
       "      <td>39.43</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1860.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>741.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>1.8672</td>\n",
       "      <td>84700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>-121.24</td>\n",
       "      <td>39.37</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2785.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>530.0</td>\n",
       "      <td>2.3886</td>\n",
       "      <td>89400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housingMedianAge  totalRooms  totalBedrooms  \\\n",
       "0        -122.23     37.88              41.0       880.0          129.0   \n",
       "1        -122.22     37.86              21.0      7099.0         1106.0   \n",
       "2        -122.24     37.85              52.0      1467.0          190.0   \n",
       "3        -122.25     37.85              52.0      1274.0          235.0   \n",
       "4        -122.25     37.85              52.0      1627.0          280.0   \n",
       "...          ...       ...               ...         ...            ...   \n",
       "20635    -121.09     39.48              25.0      1665.0          374.0   \n",
       "20636    -121.21     39.49              18.0       697.0          150.0   \n",
       "20637    -121.22     39.43              17.0      2254.0          485.0   \n",
       "20638    -121.32     39.43              18.0      1860.0          409.0   \n",
       "20639    -121.24     39.37              16.0      2785.0          616.0   \n",
       "\n",
       "       population  households  medianIncome  medianHouseValue  \n",
       "0           322.0       126.0        8.3252          452600.0  \n",
       "1          2401.0      1138.0        8.3014          358500.0  \n",
       "2           496.0       177.0        7.2574          352100.0  \n",
       "3           558.0       219.0        5.6431          341300.0  \n",
       "4           565.0       259.0        3.8462          342200.0  \n",
       "...           ...         ...           ...               ...  \n",
       "20635       845.0       330.0        1.5603           78100.0  \n",
       "20636       356.0       114.0        2.5568           77100.0  \n",
       "20637      1007.0       433.0        1.7000           92300.0  \n",
       "20638       741.0       349.0        1.8672           84700.0  \n",
       "20639      1387.0       530.0        2.3886           89400.0  \n",
       "\n",
       "[20640 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_columns\", 500)\n",
    "cal_housing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa que las distintas columnas tienen valores en rangos diferentes. Por ejemplo, los valores de la columna objetivo ``medianHouseValue`` son órdenes de magnitud superiores a los de otras columnas. Esta diferencia en la escala a menudo causa problemas cuando se entrena un modelo ML, por lo que es una práctica común hacer feature engineering para normalizar o estandarizar esos valores (dependiendo de la naturaleza de la distribución numérica y la presencia de valores atípicos).\n",
    "\n",
    "Sin embargo, dado que Autopilot se encarga de hacer feature engineering automáticamente (entre otras cosas), no vamos a realizar ningún análisis o transformación nosotros mismos.\n",
    "\n",
    "Para ilustrar también cómo Autopilot maneja problemas en los datos, como los valores nulos, introduzcamos algunos valores vacíos aleatorios en nuestro conjunto de datos para la columna de la edad media de la vivienda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomamos una pequeña muestra aleatoriamente del Dataset\n",
    "cal_housing_df_sample = cal_housing_df.sample(frac=0.01, random_state=100)\n",
    "\n",
    "cal_housing_df[\"housingMedianAge\"].loc[cal_housing_df_sample.index] = (\n",
    "    cal_housing_df[\"housingMedianAge\"].loc[cal_housing_df_sample.index].apply(lambda row: np.nan)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora el Dataset debería contener valores nulos para ~1% de las filas en la columna `housingMedianAge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206 values removed from housingMedianAge column\n"
     ]
    }
   ],
   "source": [
    "isna_sum = cal_housing_df[\"housingMedianAge\"].isna().sum()\n",
    "\n",
    "print(f\"{isna_sum} values removed from housingMedianAge column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos los datos para entrenamiento y testing (80/20 split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = cal_housing_df.sample(frac=0.8, random_state=200)\n",
    "\n",
    "test_data = cal_housing_df.drop(train_data.index)\n",
    "\n",
    "test_data_no_target = test_data.drop(columns=[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subimos el Dataset a S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data uploaded to: s3://sagemaker-us-east-1-688013747199/sagemaker/DEMO-autopilot-housing/train/train_data.csv\n",
      "Test data uploaded to: s3://sagemaker-us-east-1-688013747199/sagemaker/DEMO-autopilot-housing/test/test_data_no_target.csv\n"
     ]
    }
   ],
   "source": [
    "train_file = \"train_data.csv\"\n",
    "train_data.to_csv(train_file, index=False, header=True)\n",
    "train_data_s3_path = session.upload_data(path=train_file, key_prefix=prefix + \"/train\")\n",
    "print(\"Train data uploaded to: \" + train_data_s3_path)\n",
    "\n",
    "test_file = \"test_data_no_target.csv\"\n",
    "test_data_no_target.to_csv(test_file, index=False, header=False)\n",
    "test_data_s3_path = session.upload_data(path=test_file, key_prefix=prefix + \"/test\")\n",
    "print(\"Test data uploaded to: \" + test_data_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup del job de SageMaker <a name=\"Settingup\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After uploading the dataset to Amazon S3, you can invoke Autopilot to find the best ML pipeline to train a model on this dataset. \n",
    "\n",
    "The required inputs for invoking an Autopilot job are:\n",
    "* Amazon S3 location for input dataset and for all output artifacts\n",
    "* Name of the target column in the dataset for predictions \n",
    "* An IAM role\n",
    "\n",
    "Currently, Autopilot supports only tabular datasets in CSV format. Either all files should have a header row, or the first file of the dataset, when sorted in alphabetical/lexical order by name, is expected to have a header row.\n",
    "\n",
    "\n",
    "Después de cargar el Dataset en Amazon S3, podemos invocar a Autopilot para encontrar el mejor pipeline para entrenar un modelo con dicho Dataset.\n",
    "\n",
    "Los inputs necesarios para invocar un job de Autopilot son:\n",
    "\n",
    "* URI de Amazon S3 para el Dataset de entrada y para todos los artefactos de salida.\n",
    "* Nombre de la columna target en el Dataset para las predicciones\n",
    "* Un rol IAM\n",
    "\n",
    "Actualmente, Autopilot solo admite Datasets tabulares en formato CSV. O bien todos los archivos deben tener una fila header, o bien se espera que el primer archivo del conjunto de datos, cuando se clasifica en orden alfabético/léxico por nombre, tenga una fila header.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as '_psutil_linux' could not be imported from 'most likely due to a circular import'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "input_data_config = [\n",
    "    {\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"S3Uri\": f\"s3://{bucket}/{prefix}/train\",\n",
    "            }\n",
    "        },\n",
    "        \"TargetAttributeName\": target,\n",
    "    }\n",
    "]\n",
    "\n",
    "job_config = {\n",
    "    \"CompletionCriteria\": {\n",
    "        \"MaxCandidates\": 2,\n",
    "        # \"MaxRuntimePerTrainingJobInSeconds\": 300,\n",
    "        \"MaxAutoMLJobRuntimeInSeconds\": 60 * 60\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "output_data_config = {\"S3OutputPath\": f\"s3://{bucket}/{prefix}/output\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos especificar el tipo de problema que queremos resolver con el Dataset (`Regresión, Multiclasificación, Clasificación Binaria`). En caso de que no estemos seguro, SageMaker Autopilot inferirá el tipo de problema basado en las estadísticas de la columna target.\n",
    "\n",
    "Como el atributo target `medianHouseValue` es una variable numérica continua, el AutoPilot inferirá el tipo de problema como regresión.\n",
    "\n",
    "Tenemos la opción de limitar el tiempo de ejecución de un trabajo de SageMaker Autopilot proporcionando el número máximo de pipelines de evaluació o candidatos (un pipeline de evaluación se llama `Candidate` porque genera un modelo candidato) o proporcionando el tiempo total asignado para el trabajo general de Autopilot. Con la configuración predeterminada, este trabajo puede tardar varias horas en ejecutarse. Esto varía entre ejecuciones debido a la naturaleza del proceso exploratorio que Autopilot utiliza para encontrar los parámetros óptimos de entrenamiento.\n",
    "\n",
    "Para esta demostración, limitamos el número de candidatos a 2 para que el trabajo termine en menos de 1 hora.\n",
    "\n",
    "Por último, también podemos desplegar el modelo que resulte \"ganador\" en un endpoint de SageMaker automáticamente al terminar. En este caso, no desplegaremos el endpoint. En su lugar, ejecutaremos un trabajo de predicción por lotes para evaluar nuestro modelo.\n",
    "\n",
    "Para obtener orientación sobre cómo configurar los parámetros del trabajo, podemos consultar la documentación del SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lanzando el job de SageMaker Autopilot<a name=\"Launching\"></a>\n",
    "\n",
    "Ahora puede lanzar el job de AutoPilot llamando a la API `create_auto_ml_job`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoMLJobName: automl-housing-20221116-19-20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AutoMLJobArn': 'arn:aws:sagemaker:us-east-1:688013747199:automl-job/automl-housing-20221116-19-20',\n",
       " 'ResponseMetadata': {'RequestId': '2e4f7058-516e-4f0e-8186-48f244834132',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '2e4f7058-516e-4f0e-8186-48f244834132',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '100',\n",
       "   'date': 'Wed, 16 Nov 2022 19:20:29 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "timestamp_suffix = strftime(\"%Y%m%d-%H-%M\", gmtime())\n",
    "\n",
    "auto_ml_job_name = \"automl-housing-\" + timestamp_suffix\n",
    "print(\"AutoMLJobName: \" + auto_ml_job_name)\n",
    "\n",
    "sm.create_auto_ml_job(\n",
    "    AutoMLJobName=auto_ml_job_name,\n",
    "    InputDataConfig=input_data_config,\n",
    "    OutputDataConfig=output_data_config,\n",
    "    AutoMLJobConfig=job_config,\n",
    "    # Uncomment to automatically deploy an endpoint\n",
    "    # ModelDeployConfig={\n",
    "    #'AutoGenerateEndpointName': True,\n",
    "    #'EndpointName': 'autopilot-DEMO-housing-' + timestamp_suffix\n",
    "    # },\n",
    "    RoleArn=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "El job de AutoPilot realizará ahora los siguientes pasos:\n",
    "\n",
    "* Análisis de datos\n",
    "* Ingeniería de features\n",
    "* Selección del modelo\n",
    "* Ajuste del modelo (optimización de hiperparámetros)\n",
    "* Modelado de la importancia de las features del modelo (SageMaker Clarify)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siiguiendo el progreso del job de SageMaker Autopilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobStatus - Secondary Status\n",
      "------------------------------\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - AnalyzingData\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - FeatureEngineering\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - ModelTuning\n",
      "InProgress - MergingAutoMLTaskReports\n",
      "InProgress - MergingAutoMLTaskReports\n",
      "InProgress - MergingAutoMLTaskReports\n",
      "InProgress - MergingAutoMLTaskReports\n",
      "InProgress - MergingAutoMLTaskReports\n",
      "InProgress - MergingAutoMLTaskReports\n",
      "InProgress - MergingAutoMLTaskReports\n",
      "InProgress - MergingAutoMLTaskReports\n",
      "InProgress - MergingAutoMLTaskReports\n",
      "InProgress - MergingAutoMLTaskReports\n",
      "Completed - Completed\n"
     ]
    }
   ],
   "source": [
    "print(\"JobStatus - Secondary Status\")\n",
    "print(\"------------------------------\")\n",
    "\n",
    "\n",
    "describe_response = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)\n",
    "print(describe_response[\"AutoMLJobStatus\"] + \" - \" + describe_response[\"AutoMLJobSecondaryStatus\"])\n",
    "job_run_status = describe_response[\"AutoMLJobStatus\"]\n",
    "\n",
    "while job_run_status not in (\"Failed\", \"Completed\", \"Stopped\"):\n",
    "    describe_response = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)\n",
    "    job_run_status = describe_response[\"AutoMLJobStatus\"]\n",
    "\n",
    "    print(\n",
    "        describe_response[\"AutoMLJobStatus\"] + \" - \" + describe_response[\"AutoMLJobSecondaryStatus\"]\n",
    "    )\n",
    "    sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "---\n",
    "## Resultados\n",
    "\n",
    "Ahora podemos utilizar la API `describe_auto_ml_job` para buscar el mejor candidato seleccionado por el trabajo de SageMaker Autopilot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "CandidateName: automl-housing-20221116-19-20uij-002-d2d17f5c\n",
      "FinalAutoMLJobObjectiveMetricName: validation:mse\n",
      "FinalAutoMLJobObjectiveMetricValue: 2977769472.0\n",
      "\n",
      "Best candidate details:: {'CandidateName': 'automl-housing-20221116-19-20uij-002-d2d17f5c', 'FinalAutoMLJobObjectiveMetric': {'MetricName': 'validation:mse', 'Value': 2977769472.0}, 'ObjectiveStatus': 'Succeeded', 'CandidateSteps': [{'CandidateStepType': 'AWS::SageMaker::ProcessingJob', 'CandidateStepArn': 'arn:aws:sagemaker:us-east-1:688013747199:processing-job/automl-housing-20221116-19-20-db-1-98da5f6194174a48a3c22f59cc59', 'CandidateStepName': 'automl-housing-20221116-19-20-db-1-98da5f6194174a48a3c22f59cc59'}, {'CandidateStepType': 'AWS::SageMaker::TrainingJob', 'CandidateStepArn': 'arn:aws:sagemaker:us-east-1:688013747199:training-job/automl-housing-20221116-19-20-dpp1-1-bf59a858abe4446a90b1c77c03', 'CandidateStepName': 'automl-housing-20221116-19-20-dpp1-1-bf59a858abe4446a90b1c77c03'}, {'CandidateStepType': 'AWS::SageMaker::TransformJob', 'CandidateStepArn': 'arn:aws:sagemaker:us-east-1:688013747199:transform-job/automl-housing-20221116-19-20-dpp1-rpb-1-051137eb63cd4877825907', 'CandidateStepName': 'automl-housing-20221116-19-20-dpp1-rpb-1-051137eb63cd4877825907'}, {'CandidateStepType': 'AWS::SageMaker::TrainingJob', 'CandidateStepArn': 'arn:aws:sagemaker:us-east-1:688013747199:training-job/automl-housing-20221116-19-20uij-002-d2d17f5c', 'CandidateStepName': 'automl-housing-20221116-19-20uij-002-d2d17f5c'}], 'CandidateStatus': 'Completed', 'InferenceContainers': [{'Image': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-sklearn-automl:2.5-1-cpu-py3', 'ModelDataUrl': 's3://sagemaker-us-east-1-688013747199/sagemaker/DEMO-autopilot-housing/output/automl-housing-20221116-19-20/data-processor-models/automl-housing-20221116-19-20-dpp1-1-bf59a858abe4446a90b1c77c03/output/model.tar.gz', 'Environment': {'AUTOML_SPARSE_ENCODE_RECORDIO_PROTOBUF': '1', 'AUTOML_TRANSFORM_MODE': 'feature-transform', 'SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT': 'application/x-recordio-protobuf', 'SAGEMAKER_PROGRAM': 'sagemaker_serve', 'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/model/code'}}, {'Image': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.3-1-cpu-py3', 'ModelDataUrl': 's3://sagemaker-us-east-1-688013747199/sagemaker/DEMO-autopilot-housing/output/automl-housing-20221116-19-20/tuning/automl-hou-dpp1-xgb/automl-housing-20221116-19-20uij-002-d2d17f5c/output/model.tar.gz', 'Environment': {'MAX_CONTENT_LENGTH': '20971520', 'SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT': 'text/csv'}}], 'CreationTime': datetime.datetime(2022, 11, 16, 19, 39, 16, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2022, 11, 16, 19, 40, 49, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 11, 16, 19, 40, 58, 982000, tzinfo=tzlocal()), 'CandidateProperties': {'CandidateArtifactLocations': {'Explainability': 's3://sagemaker-us-east-1-688013747199/sagemaker/DEMO-autopilot-housing/output/automl-housing-20221116-19-20/documentation/explainability/output', 'ModelInsights': 's3://sagemaker-us-east-1-688013747199/sagemaker/DEMO-autopilot-housing/output/automl-housing-20221116-19-20/documentation/model_monitor/output'}, 'CandidateMetrics': [{'MetricName': 'MAE', 'Value': 37005.0859375, 'Set': 'Validation', 'StandardMetricName': 'MAE'}, {'MetricName': 'RMSE', 'Value': 54555.55078125, 'Set': 'Validation', 'StandardMetricName': 'RMSE'}, {'MetricName': 'MSE', 'Value': 2977769472.0, 'Set': 'Validation', 'StandardMetricName': 'MSE'}, {'MetricName': 'R2', 'Value': 0.7779200077056885, 'Set': 'Validation', 'StandardMetricName': 'R2'}]}}\n"
     ]
    }
   ],
   "source": [
    "best_candidate = sm.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name)[\"BestCandidate\"]\n",
    "best_candidate_name = best_candidate[\"CandidateName\"]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"CandidateName: \" + best_candidate_name)\n",
    "print(\n",
    "    \"FinalAutoMLJobObjectiveMetricName: \"\n",
    "    + best_candidate[\"FinalAutoMLJobObjectiveMetric\"][\"MetricName\"]\n",
    ")\n",
    "print(\n",
    "    \"FinalAutoMLJobObjectiveMetricValue: \"\n",
    "    + str(best_candidate[\"FinalAutoMLJobObjectiveMetric\"][\"Value\"])\n",
    ")\n",
    "print(\"\\nBest candidate details:: \" + str(best_candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si sentimos curiosidad por explorar el rendimiento de los otros algoritmos explorados por AutoPilot, podemos enumerarlos llamando a la API `list_candidates_for_auto_ml_job`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "automl-housing-20221116-19-20uij-002-d2d17f5c {'MetricName': 'validation:mse', 'Value': 2977769472.0}\n",
      "683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.3-1-cpu-py3 \n",
      "\n",
      "automl-housing-20221116-19-20uij-001-4b62271a {'MetricName': 'validation:mse', 'Value': 3039514368.0}\n",
      "683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.3-1-cpu-py3 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sm_dict = sm.list_candidates_for_auto_ml_job(AutoMLJobName=auto_ml_job_name)\n",
    "for item in sm_dict[\"Candidates\"]:\n",
    "    print(item[\"CandidateName\"], item[\"FinalAutoMLJobObjectiveMetric\"])\n",
    "    print(item[\"InferenceContainers\"][1][\"Image\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "AutoPilot genera automáticamente dos Jupyter Notebooks:  \n",
    "\n",
    "- `SageMakerAutopilotDataExplorationNotebook.ipynb`\n",
    "- `SageMakerAutopilotCandidateDefinitionNotebook.ipynb`\n",
    "\n",
    "Estos notebooks se guardan en S3. Vamos a descargarlos en nuestra instancia de SageMaker para explorarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-688013747199 sagemaker/DEMO-autopilot-housing/output/automl-housing-20221116-19-20/sagemaker-automl-candidates/automl-housing-20221116-19-20-pr-1-494dabd5d26444208726a619d64f/notebooks/SageMakerAutopilotCandidateDefinitionNotebook.ipynb sagemaker/DEMO-autopilot-housing/output/automl-housing-20221116-19-20/sagemaker-automl-candidates/automl-housing-20221116-19-20-pr-1-494dabd5d26444208726a619d64f/notebooks/SageMakerAutopilotDataExplorationNotebook.ipynb\n"
     ]
    }
   ],
   "source": [
    "candidate_nbk = describe_response[\"AutoMLJobArtifacts\"][\"CandidateDefinitionNotebookLocation\"]\n",
    "data_explore_nbk = describe_response[\"AutoMLJobArtifacts\"][\"DataExplorationNotebookLocation\"]\n",
    "\n",
    "\n",
    "def split_s3_path(s3_path):\n",
    "    path_parts = s3_path.replace(\"s3://\", \"\").split(\"/\")\n",
    "    bucket = path_parts.pop(0)\n",
    "    key = \"/\".join(path_parts)\n",
    "    return bucket, key\n",
    "\n",
    "\n",
    "s3_bucket, candidate_nbk_key = split_s3_path(candidate_nbk)\n",
    "_, data_explore_nbk_key = split_s3_path(data_explore_nbk)\n",
    "\n",
    "print(s3_bucket, candidate_nbk_key, data_explore_nbk_key)\n",
    "\n",
    "session.download_data(path=\"./\", bucket=s3_bucket, key_prefix=candidate_nbk_key)\n",
    "\n",
    "session.download_data(path=\"./\", bucket=s3_bucket, key_prefix=data_explore_nbk_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebooks candidatos durante la exploración de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vamos a inspeccionar el cuaderno de exploración de datos. Comprueba la sección `Column Analysis and Descriptive Statistics` para ver el análisis realizado por Autopilot.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ahora inspeccionemos el notebook Candidate Generation. Comprueba la sección  `Generated Candidates` para ver los diferentes algoritmos y estrategias de transformación de datos utilizados por Autopilot.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Autopilot también genera automáticamente un reporte sobre la importancia de las features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainability_prefix = best_candidate[\"CandidateProperties\"][\"CandidateArtifactLocations\"][\n",
    "    \"Explainability\"\n",
    "]\n",
    "\n",
    "s3_bucket, explainability_dir = split_s3_path(explainability_prefix)\n",
    "\n",
    "session.download_data(path=\"./\", bucket=s3_bucket, key_prefix=explainability_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código anterior descargará un directorio en nuestro entorno local. En ese directorio (el prefijo es el nombre del job de AutoPilot, el sufijo se genera automáticamente), debería ver los artefactos de SageMaker Clarify. SageMaker Clarify proporciona una mayor visibilidad de los datos y modelos de entrenamiento para identificar y limitar el sesgo y explicar las predicciones. El siguiente código abrirá el informe de importancia de la característica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"700\"\n",
       "            height=\"600\"\n",
       "            src=\"automl-housing-20221116-19-20uij-002-d2d17f5c/report.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f61d0fa2d50>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "# Obtener el nombre de directorio autogenerado para los artefactos de SageMaker Clarify\n",
    "dir_name = (\n",
    "    session.list_s3_files(bucket=s3_bucket, key_prefix=explainability_dir)[0]\n",
    "    .replace(explainability_dir, \"\")\n",
    "    .split(\"/\")[1]\n",
    ")\n",
    "\n",
    "# Mostrar informe HTML\n",
    "IFrame(src=f\"{dir_name}/report.html\", width=700, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your results may vary. But you're likely to see latitude and longitude (i.e., location) on top, along with population size and median income, which are stronger predictors of housing prices than the other features in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SageMaker Studio, you can also navigate to SageMaker resources tab, click on Experiments and trials, and find your Autopilot experiment. You can double-click on the experiment name to list all trials, and from there you can double-click on a specific trial to see its details, including charts and metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluate Model Using Test Dataset<a name=\"Evaluate\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model on previously unseen data, we will test it against the test dataset we prepared earlier. For that, we don't necessarily need to deploy the model to an endpoint, we can simply run a batch transform job to get predictions for our unlabeled test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Transform Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import AutoML\n",
    "\n",
    "automl = AutoML.attach(auto_ml_job_name=auto_ml_job_name)\n",
    "\n",
    "s3_transform_output_path = \"s3://{}/{}/inference-results/\".format(s3_bucket, prefix)\n",
    "\n",
    "model_name = \"{0}-model\".format(best_candidate_name)\n",
    "\n",
    "model = automl.create_model(\n",
    "    name=model_name,\n",
    "    candidate=best_candidate,\n",
    ")\n",
    "\n",
    "output_path = s3_transform_output_path + best_candidate_name + \"/\"\n",
    "\n",
    "transformer = model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    assemble_with=\"Line\",\n",
    "    strategy=\"SingleRecord\",\n",
    "    output_path=output_path,\n",
    "    env={\"SAGEMAKER_MODEL_SERVER_TIMEOUT\": \"100\", \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Transform Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting transform job automl-housing-20221116-19-20uij-002-d2-2022-11-16-20-00-33-113\n"
     ]
    }
   ],
   "source": [
    "transformer.transform(\n",
    "    data=test_data_s3_path,\n",
    "    split_type=\"Line\",\n",
    "    content_type=\"text/csv\",\n",
    "    wait=False,\n",
    "    model_client_config={\"InvocationsTimeoutInSeconds\": 80, \"InvocationsMaxRetries\": 1},\n",
    ")\n",
    "\n",
    "print(\"Starting transform job {}\".format(transformer._current_job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Track Transform Job Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "automl-housing-20221116-19-20uij-002-d2-2022-11-16-20-00-33-113 transform job is running.\n",
      "automl-housing-20221116-19-20uij-002-d2-2022-11-16-20-00-33-113 transform job is running.\n",
      "automl-housing-20221116-19-20uij-002-d2-2022-11-16-20-00-33-113 transform job is running.\n",
      "automl-housing-20221116-19-20uij-002-d2-2022-11-16-20-00-33-113 transform job is running.\n",
      "automl-housing-20221116-19-20uij-002-d2-2022-11-16-20-00-33-113 transform job is running.\n",
      "automl-housing-20221116-19-20uij-002-d2-2022-11-16-20-00-33-113 transform job is running.\n",
      "automl-housing-20221116-19-20uij-002-d2-2022-11-16-20-00-33-113 transform job is running.\n",
      "\n",
      "Completed.\n"
     ]
    }
   ],
   "source": [
    "## Wait for jobs to finish\n",
    "pending_complete = True\n",
    "job_name = transformer._current_job_name\n",
    "\n",
    "while pending_complete:\n",
    "    pending_complete = False\n",
    "\n",
    "    description = sm.describe_transform_job(TransformJobName=job_name)\n",
    "    if description[\"TransformJobStatus\"] not in [\"Failed\", \"Completed\"]:\n",
    "        pending_complete = True\n",
    "\n",
    "    print(\"{} transform job is running.\".format(job_name))\n",
    "    time.sleep(60)\n",
    "\n",
    "print(\"\\nCompleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Inference Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transform job will have now generated a CSV file with inference results for the test dataset. We will use those results and compare them with the real test labels to see how the model performs compared to real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_from_s3(s3uri, file_name):\n",
    "    parsed_url = urlparse(s3uri)\n",
    "    bucket_name = parsed_url.netloc\n",
    "    prefix = parsed_url.path[1:].strip(\"/\")\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    obj = s3.Object(bucket_name, \"{}/{}\".format(prefix, file_name))\n",
    "    return obj.get()[\"Body\"].read().decode(\"utf-8\")\n",
    "\n",
    "\n",
    "job_status = sm.describe_transform_job(TransformJobName=job_name)[\"TransformJobStatus\"]\n",
    "\n",
    "if job_status == \"Completed\":\n",
    "    pred_csv = get_csv_from_s3(transformer.output_path, \"{}.out\".format(test_file))\n",
    "    predictions = pd.read_csv(io.StringIO(pred_csv), header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2035430611.447957\n",
      "RMSE: 45115.746823564354\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "labels_df = test_data[target]\n",
    "mse = mean_squared_error(labels_df, predictions)\n",
    "rmse = sqrt(mse)\n",
    "\n",
    "print(\"MSE: {0}\\nRMSE: {1}\".format(mse, rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup\n",
    "\n",
    "The Autopilot job creates many underlying artifacts such as dataset splits, preprocessing scripts, or preprocessed data, etc. This code, when un-commented, deletes them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\"s3\")\n",
    "s3_bucket = s3.Bucket(bucket)\n",
    "\n",
    "print(s3_bucket)\n",
    "job_outputs_prefix = \"{}/output/{}\".format(prefix, auto_ml_job_name)\n",
    "print(job_outputs_prefix)\n",
    "\n",
    "# Delete S3 objects\n",
    "s3_bucket.objects.filter(Prefix=job_outputs_prefix).delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then delete all the experiment and model resources created by the Autopilot experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_experiment_resources(experiment_name):\n",
    "    trials = sm.list_trials(ExperimentName=experiment_name)[\"TrialSummaries\"]\n",
    "    print(\"TrialNames:\")\n",
    "    for trial in trials:\n",
    "        trial_name = trial[\"TrialName\"]\n",
    "        print(f\"\\n{trial_name}\")\n",
    "\n",
    "        components_in_trial = sm.list_trial_components(TrialName=trial_name)\n",
    "        print(\"\\tTrialComponentNames:\")\n",
    "        for component in components_in_trial[\"TrialComponentSummaries\"]:\n",
    "            component_name = component[\"TrialComponentName\"]\n",
    "            print(f\"\\t{component_name}\")\n",
    "            sm.disassociate_trial_component(TrialComponentName=component_name, TrialName=trial_name)\n",
    "            try:\n",
    "                # comment out to keep trial components\n",
    "                sm.delete_trial_component(TrialComponentName=component_name)\n",
    "            except:\n",
    "                # component is associated with another trial\n",
    "                continue\n",
    "            # to prevent throttling\n",
    "            time.sleep(5)\n",
    "        sm.delete_trial(TrialName=trial_name)\n",
    "    sm.delete_experiment(ExperimentName=experiment_name)\n",
    "    print(f\"\\nExperiment {experiment_name} deleted\")\n",
    "\n",
    "\n",
    "def cleanup_autopilot_models(autopilot_job_name):\n",
    "    print(\"{0}:\\n\".format(autopilot_job_name))\n",
    "    response = sm.list_models(NameContains=autopilot_job_name)\n",
    "\n",
    "    for model in response[\"Models\"]:\n",
    "        model_name = model[\"ModelName\"]\n",
    "        print(f\"\\t{model_name}\")\n",
    "        sm.delete_model(ModelName=model_name)\n",
    "        # to prevent throttling\n",
    "        time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_experiment_resources(\"{0}-aws-auto-ml-job\".format(auto_ml_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_autopilot_models(auto_ml_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the following code, when uncommented, will delete the local files used in this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "def delete_local_files():\n",
    "    base_path = \"\"\n",
    "    dir_list = glob.iglob(os.path.join(base_path, \"{0}*\".format(auto_ml_job_name)))\n",
    "\n",
    "    for path in dir_list:\n",
    "        if os.path.isdir(path):\n",
    "            shutil.rmtree(path)\n",
    "\n",
    "    if os.path.exists(\"CaliforniaHousing\"):\n",
    "        shutil.rmtree(\"CaliforniaHousing\")\n",
    "\n",
    "    if os.path.exists(\"cal_housing.tgz\"):\n",
    "        os.remove(\"cal_housing.tgz\")\n",
    "\n",
    "    if os.path.exists(\"SageMakerAutopilotCandidateDefinitionNotebook.ipynb\"):\n",
    "        os.remove(\"SageMakerAutopilotCandidateDefinitionNotebook.ipynb\")\n",
    "\n",
    "    if os.path.exists(\"SageMakerAutopilotDataExplorationNotebook.ipynb\"):\n",
    "        os.remove(\"SageMakerAutopilotDataExplorationNotebook.ipynb\")\n",
    "\n",
    "    if os.path.exists(\"test_data_no_target.csv\"):\n",
    "        os.remove(\"test_data_no_target.csv\")\n",
    "\n",
    "    if os.path.exists(\"test_data.csv\"):\n",
    "        os.remove(\"test_data.csv\")\n",
    "\n",
    "    if os.path.exists(\"train_data.csv\"):\n",
    "        os.remove(\"train_data.csv\")\n",
    "\n",
    "\n",
    "## UNCOMMENT TO CLEAN UP LOCAL FILES\n",
    "# delete_local_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: If you enabled automatic endpoint creation, you will need to delete the endpoint manually.**"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3.9.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

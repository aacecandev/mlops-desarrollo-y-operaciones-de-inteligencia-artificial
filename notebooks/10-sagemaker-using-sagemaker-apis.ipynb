{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Transformación dee los datos y entrenamiento d3el modelo ysando trabajos de entrenamiento gestionados por SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este módulo utilizaremos el mismo Dataset y modelo, pero actualizaremos el modelo para utilizar las características de SageMaker para escalar la transformación del Dataset y el entrenamiento del modelo más allá de Jupyter notebook.\n",
    "\n",
    "Este notebook incluye todos los pasos clave, como el preprocesamiento de datos con SageMaker Processing, y el entrenamiento y despliegue del modelo con  entrenamiento e inferencia alojados en SageMaker. El tuning automático del modelo en SageMaker se utiliza para ajustar los hiperparámetros del modelo. Si utilizamos TensorFlow 2, podemos utilizar el contenedor del framework de TensorFlow 2 preconstruido de Amazon SageMaker con scripts de entrenamiento similares a los que utilizaríamos fuera de SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet --disable-pip-version-check sagemaker=='v2.90.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import sagemaker\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = sagemaker.session.Session()\n",
    "bucket = sess.default_bucket() \n",
    "region = boto3.Session().region_name\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "train_dir = os.path.join(os.getcwd(), 'data/train')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "test_dir = os.path.join(os.getcwd(), 'data/test')\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "raw_dir = os.path.join(os.getcwd(), 'data/raw')\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "\n",
    "batch_dir = os.path.join(os.getcwd(), 'data/batch')\n",
    "os.makedirs(batch_dir, exist_ok=True)\n",
    "\n",
    "print(f'SageMaker Version: {sagemaker.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## SageMaker Processing para transformación del dataset <a class=\"anchor\" id=\"SageMakerProcessing\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, importaremos el Dataset y lo transformaremos con SageMaker Processing, que puede utilizarse para procesar terabytes de datos en un clúster gestionado por SageMaker separado de la instancia que ejecuta nuestro Jupyter Kernel Gateway. En un flujo de trabajo típico de SageMaker, los notebooks sólo se utilizan para la creación de prototipos y pueden ejecutarse en instancias relativamente baratas y menos potentes, mientras que las tareas de procesamiento, formación y alojamiento de modelos se ejecutan en instancias separadas y más potentes gestionadas por SageMaker.\n",
    "\n",
    "SageMaker Processing incluye soporte off-the-shelf para Scikit-learn, así como una opción de Bring Your Own Container (BYOC), por lo que se puede utilizar con muchas tecnologías para transformación de datos y tareas diferentes. Una alternativa a SageMaker Processing es [SageMaker Data Wrangler](https://aws.amazon.com/sagemaker/data-wrangler/), una herramienta visual de preparación de datos integrada en la interfaz de usuario de SageMaker Studio.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para trabajar con SageMaker Processing, primero cargaremos el Dataset de Boston Housing, guardaremos las features en bruto y lo subiremos a Amazon S3 para que SageMaker Processing pueda acceder a ellos.\n",
    "\n",
    "También guardaremos las etiquetas para el entrenamiento y las pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.python.keras.datasets import boston_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "np.save(os.path.join(raw_dir, 'x_train.npy'), x_train)\n",
    "np.save(os.path.join(raw_dir, 'x_test.npy'), x_test)\n",
    "np.save(os.path.join(raw_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(raw_dir, 'y_test.npy'), y_test)\n",
    "\n",
    "s3_prefix = 'tf-2-workflow'\n",
    "rawdata_s3_prefix = f'{s3_prefix}/data/raw'\n",
    "raw_s3 = sess.upload_data(path='./data/raw/', key_prefix=rawdata_s3_prefix)\n",
    "\n",
    "print(raw_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, simplemente suministramos un script de preprocesamiento de datos en Python.  Para este ejemplo, estamos usando un contenedor con el framework Scikit-learn pre-construido por SageMaker, que incluye muchas funciones comunes para el procesamiento de datos. Hay pocas limitaciones en cuanto a los tipos de código y operaciones que se pueden ejecutar, y sólo un **contrato mínimo con la API: los datos de entrada y salida deben colocarse en directorios específicos**. Si se hace esto, SageMaker Processing carga automáticamente los datos de entrada desde S3 y carga los datos transformados de vuelta en S3 cuando el trabajo ha concluído."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### NOTE - SageMaker Local Mode\n",
    "\n",
    "Para ayudarnos a realizar los cambios de código necesarios para adaptar nuestros script a SageMaker, podemos utilizar el [modo local de SageMaker](https://aws.amazon.com/blogs/machine-learning/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance/) para procesar, entrenar o inferir directamente desde nuestra máquina local.\n",
    "\n",
    "El modo local del SDK de Python de Amazon SageMaker puede emular entrenamientos de SageMaker en la CPU (una o varias instancias) y en la GPU (una sola instancia) cambiando un solo argumento en los estimadores de TensorFlow, PyTorch o MXNet. Para ello, utiliza Docker compose y NVIDIA Docker. También extraerá los contenedores de Amazon SageMaker TensorFlow, PyTorch o MXNet de Amazon ECR, por lo que necesitarás poder acceder a un repositorio público de Amazon ECR desde tu entorno local.\n",
    "\n",
    "En este notebook no utilizaremos el modo local de SageMaker, ya que tenemos listos los scripts de procesamiento, entrenamiento y evaluación. Puedes consultar [este repositorio de GitHub](https://github.com/aws-samples/amazon-sagemaker-local-mode) que contiene ejemplos y recursos relacionados que muestran cómo preprocesar, entrenar, depurar tu script de entrenamiento con breakpoint y servir en tu máquina local utilizando el modo local de Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    input_files = glob.glob(\"/opt/ml/processing/input/*.npy\")\n",
    "    print(f'\\nINPUT FILE LIST: \\n{input_files}\\n')\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    x_train = np.load(os.path.join('/opt/ml/processing/input', 'x_train.npy'))\n",
    "    scaler.fit(x_train)\n",
    "    \n",
    "    for file in input_files:\n",
    "        raw = np.load(file)\n",
    "        # Solo transformamos las columnas de features\n",
    "        if 'y_' not in file:\n",
    "            transformed = scaler.transform(raw)\n",
    "        if 'train' in file:\n",
    "            if 'y_' in file:\n",
    "                output_path = os.path.join('/opt/ml/processing/train', 'y_train.npy')\n",
    "                np.save(output_path, raw)\n",
    "                print('SAVED LABEL TRAINING DATA FILE\\n')\n",
    "            else:\n",
    "                output_path = os.path.join('/opt/ml/processing/train', 'x_train.npy')\n",
    "                np.save(output_path, transformed)\n",
    "                print('SAVED TRANSFORMED TRAINING DATA FILE\\n')\n",
    "        else:\n",
    "            if 'y_' in file:\n",
    "                output_path = os.path.join('/opt/ml/processing/test', 'y_test.npy')\n",
    "                np.save(output_path, raw)\n",
    "                print('SAVED LABEL TEST DATA FILE\\n')\n",
    "            else:\n",
    "                output_path = os.path.join('/opt/ml/processing/test', 'x_test.npy')\n",
    "                np.save(output_path, transformed)\n",
    "                print('SAVED TRANSFORMED TEST DATA FILE\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from model_def import get_model\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script\n",
    "    parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--batch_size', type=int, default=64)\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.1)\n",
    "\n",
    "    # data directories\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "\n",
    "    # model directory\n",
    "    parser.add_argument('--sm-model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "\n",
    "def get_train_data(train_dir):\n",
    "\n",
    "    x_train = np.load(os.path.join(train_dir, 'x_train.npy'))\n",
    "    y_train = np.load(os.path.join(train_dir, 'y_train.npy'))\n",
    "    print('x train', x_train.shape,'y train', y_train.shape)\n",
    "\n",
    "    return x_train, y_train\n",
    "\n",
    "\n",
    "def get_test_data(test_dir):\n",
    "\n",
    "    x_test = np.load(os.path.join(test_dir, 'x_test.npy'))\n",
    "    y_test = np.load(os.path.join(test_dir, 'y_test.npy'))\n",
    "    print('x test', x_test.shape,'y test', y_test.shape)\n",
    "\n",
    "    return x_test, y_test\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    args, _ = parse_args()\n",
    "\n",
    "    print('Training data location: {}'.format(args.train))\n",
    "    print('Test data location: {}'.format(args.test))\n",
    "    x_train, y_train = get_train_data(args.train)\n",
    "    x_test, y_test = get_test_data(args.test)\n",
    "\n",
    "    device = '/cpu:0'\n",
    "    print(device)\n",
    "    batch_size = args.batch_size\n",
    "    epochs = args.epochs\n",
    "    learning_rate = args.learning_rate\n",
    "    print('batch_size = {}, epochs = {}, learning rate = {}'.format(batch_size, epochs, learning_rate))\n",
    "\n",
    "    with tf.device(device):\n",
    "\n",
    "        model = get_model()\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='mse')\n",
    "        model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "                  validation_data=(x_test, y_test))\n",
    "\n",
    "        # evaluate on test set\n",
    "        scores = model.evaluate(x_test, y_test, batch_size, verbose=2)\n",
    "        print(\"\\nTest MSE :\", scores)\n",
    "\n",
    "        # save model\n",
    "        model.save(args.sm_model_dir + '/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model_def.py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def get_model():\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(13,))\n",
    "    hidden_1 = tf.keras.layers.Dense(13, activation='tanh')(inputs)\n",
    "    hidden_2 = tf.keras.layers.Dense(6, activation='sigmoid')(hidden_1)\n",
    "    outputs = tf.keras.layers.Dense(1)(hidden_2)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Antes de arrancar el trabajo de procesamiento de SageMaker, instanciamos un objeto `SKLearnProcessor`. Este objeto permite especificar el tipo de instancia a utilizar en el trabajo, así como el número de instancias. Arrancar un cluster es sólo cuestión de establecer `instance_count` a 2 o más, pero nuestra transformación tiene un `StandardScaler` que debe ejecutarse sobre todos los datos de entrenamiento y aplicarse por igual a los datos de entrenamiento y de test. Esto no se puede paralelizar con `scikit-learn`, pero como el Dataset es pequeño, no supone problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "try:\n",
    "    execution_role = get_execution_role()\n",
    "except ValueError:\n",
    "    execution_role = \"AmazonSageMaker-ExecutionRole-20191003T111555\"\n",
    "\n",
    "sklearn_processor1 = SKLearnProcessor(\n",
    "  framework_version='0.23-1',\n",
    "  role=execution_role,\n",
    "  instance_type='ml.m5.xlarge',\n",
    "  #instance_count=2\n",
    "  instance_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Ahora estamos listos para ejecutar el trabajo de procesamiento.\n",
    "\n",
    "Para permitir la distribución de los archivos de datos por igual entre las instancias, podríamosespecificar el tipo de distribución `ShardedByS3Key` en el objeto `ProcessingInput`. Esto aseguraría que si tienes `n` instancias, cada instancia recibirá `1/n` archivos del bucket S3 especificado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda de código puede tardar unos 5 o 10 minutos en ejecutarse, principalmente para configurar el clúster. Podemos revisar el trabajo en la consola Web, en la sección Processing Jobs.\n",
    "\n",
    "Al final del trabajo, el cluster será desmontado automáticamente por SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from time import gmtime, strftime \n",
    "\n",
    "processing_job_name = f'tf-2-workflow-{strftime(\"%d-%H-%M-%S\", gmtime())}'\n",
    "output_destination = f's3://{bucket}/{s3_prefix}/data'\n",
    "\n",
    "sklearn_processor1.run(\n",
    "    code='preprocessing.py',\n",
    "    job_name=processing_job_name,\n",
    "    inputs=[ProcessingInput(\n",
    "        source=raw_s3,\n",
    "        destination='/opt/ml/processing/input',\n",
    "        #s3_data_distribution_type='ShardedByS3Key'\n",
    "    )],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name='train',\n",
    "            destination=f'{output_destination}/train',\n",
    "            source='/opt/ml/processing/train'),\n",
    "        ProcessingOutput(output_name='test',\n",
    "            destination=f'{output_destination}/test',\n",
    "            source='/opt/ml/processing/test')\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessing_job_description = sklearn_processor1.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividiendo carga entre varias instancias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**SOLO CON MÁS DE UNA INSTANCIA Y AÑADIENDO EL PARÁMETRO COMENTADO**\n",
    "\n",
    "En la salida del log del trabajo de Procesamiento de SageMaker de arriba, podemos ver registros diferentes para las dos instancias, ya que cada instancia recibió archivos diferentes. \n",
    "\n",
    "Sin el tipo de distribución `ShardedByS3Key`, cada instancia recibiría una copia de **todos** los archivos.  Al repartir los datos de forma equitativa entre `n` instancias, deberíamos percibir un aumento de velocidad de aproximadamente un factor de `n` para la mayoría de las transformaciones de datos stateless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Después de guardar los resultados del trabajo localmente, pasaremos al código de entrenamiento e inferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_train_in_s3 = f'{output_destination}/train/x_train.npy'\n",
    "y_train_in_s3 = f'{output_destination}/train/y_train.npy'\n",
    "x_test_in_s3 = f'{output_destination}/test/x_test.npy'\n",
    "y_test_in_s3 = f'{output_destination}/test/y_test.npy'\n",
    "\n",
    "!aws s3 cp {x_train_in_s3} ./data/train/x_train.npy\n",
    "!aws s3 cp {y_train_in_s3} ./data/train/y_train.npy\n",
    "!aws s3 cp {x_test_in_s3} ./data/test/x_test.npy\n",
    "!aws s3 cp {y_test_in_s3} ./data/test/y_test.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Entrenamiento alojado en SageMaker <a class=\"anchor\" id=\"SageMakerHostedTraining\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Ahora que hemos preparado Dataset, podemos pasar a la funcionalidad de entrenamiento del modelo de SageMaker.\n",
    "\n",
    "Con el entrenamiento alojado de SageMaker, el entrenamiento en sí mismo no ocurre en la instancia del notebook tampoco, sino en un clúster separado de máquinas gestionadas por SageMaker. Antes de comenzar el entrenamiento alojado, los datos deben estar en S3, o en un sistema de archivos EFS o FSx para Lustre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora subiremos a S3, y confirmaremos que la subida fue exitosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s3_prefix = 'tf-2-workflow'\n",
    "\n",
    "traindata_s3_prefix = f'{s3_prefix}/data/train'\n",
    "testdata_s3_prefix = f'{s3_prefix}/data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_s3 = sess.upload_data(path='./data/train/', key_prefix=traindata_s3_prefix)\n",
    "test_s3 = sess.upload_data(path='./data/test/', key_prefix=testdata_s3_prefix)\n",
    "\n",
    "inputs = {'train':train_s3, 'test': test_s3}\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Ahora podemos configurar un objeto `Estimator` para el entrenamiento alojado. Simplemente llamamos a `fit` para iniciar el entrenamiento alojado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "train_instance_type = 'ml.m5.xlarge'\n",
    "hyperparameters = {'epochs': 70, 'batch_size': 128, 'learning_rate': 0.01}\n",
    "\n",
    "hosted_estimator = TensorFlow(\n",
    "  source_dir='./',\n",
    "  entry_point='train.py',\n",
    "  instance_type=train_instance_type,\n",
    "  instance_count=1,\n",
    "  hyperparameters=hyperparameters,\n",
    "  role=sagemaker.get_execution_role(),\n",
    "  base_job_name='tf-2-workflow',\n",
    "  framework_version='2.3.1',\n",
    "  py_version='py37'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Después de iniciar el trabajo de entrenamiento alojado con la llamada al método `fit` que aparece a continuación, deberíamos observar que el validation loss converge con cada epoch. ¿Podemos hacerlo mejor? Veremos una forma de hacerlo en el notebook **Ajuste automático del modelo** más adelante.\n",
    "\n",
    "Mientras tanto, el trabajo de entrenamiento alojado debería tardar unos 3 minutos en completarse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hosted_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "El trabajo de entrenamiento produce un modelo guardado en S3 que podemos obtener. Este es un ejemplo de la modularidad de SageMaker: habiendo entrenado el modelo en SageMaker, ahora puedes sacar el modelo de SageMaker y ejecutarlo en cualquier otro lugar. Alternativamente, podemos desplegar el modelo en un entorno listo para producción utilizando la funcionalidad de endpoints alojados de SageMaker, como se muestra en la sección **Endpoint alojado en SageMaker** a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuperar el modelo de S3 es muy fácil: el estimador de entrenamiento alojado que creó anteriormente almacena una referencia a la ubicación del modelo en S3.  Sólo tienes que copiar el modelo desde S3 utilizando la propiedad `model_data` del estimador y descomprimirlo para inspeccionar el contenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!aws s3 cp {hosted_estimator.model_data} ./model/model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "El archivo descomprimido debe incluir los assets requeridos por TensorFlow Serving para cargar el modelo y servirlo, incluyendo un archivo .pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!tar -xvzf ./model/model.tar.gz -C ./model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Model Tuning <a class=\"anchor\" id=\"AutomaticModelTuning\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Hasta ahora nos hemos limitado a ejecutar un trabajo de entrenamiento en host sin ningún intento real de ajustar los hiperparámetros para producir un modelo mejor. Seleccionar los valores correctos de los hiperparámetros para entrenar el modelo puede ser difícil, y normalmente lleva mucho tiempo si se hace manualmente. La combinación correcta de hiperparámetros depende de sus datos y algoritmo; algunos algoritmos tienen muchos hiperparámetros diferentes que pueden ser ajustados. Algunos son muy sensibles a los valores de hiperparámetros seleccionados. Y la mayoría tienen una relación no lineal entre el ajuste del modelo y los valores de los hiperparámetros.\n",
    "\n",
    "SageMaker Automatic Model Tuning ayuda a automatizar el proceso de ajuste de hiperparámetros: ejecuta múltiples trabajos de entrenamiento con diferentes combinaciones de hiperparámetros para encontrar el conjunto con el mejor rendimiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos especificando los hiperparámetros que deseamos ajustar, y el rango de valores sobre el que ajustar cada uno. También debemos especificar una métrica objetivo a optimizar: en este caso de uso, nos gustaría minimizar el validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "  'learning_rate': ContinuousParameter(0.001, 0.2, scaling_type=\"Logarithmic\"),\n",
    "  'epochs': IntegerParameter(10, 50),\n",
    "  'batch_size': IntegerParameter(64, 256),\n",
    "}\n",
    "\n",
    "metric_definitions = [\n",
    "  {\n",
    "    'Name': 'loss',\n",
    "    'Regex': ' loss: ([0-9\\\\.]+)'\n",
    "  },\n",
    "  {\n",
    "    'Name': 'val_loss',\n",
    "    'Regex': ' val_loss: ([0-9\\\\.]+)'\n",
    "  }\n",
    "]\n",
    "\n",
    "objective_metric_name = 'val_loss'\n",
    "objective_type = 'Minimize'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A continuación especificamos un objeto `HyperparameterTuner` que toma las definiciones anteriores como parámetros. Cada trabajo de ajuste debe tener un budget o número máximo de trabajos de entrenamiento. Un trabajo de ajuste se completará después de que se hayan ejecutado todos esos trabajos de entrenamiento.\n",
    "\n",
    "También podemos especificar cuánto paralelismo emplear, en este caso tres trabajos, lo que significa que el trabajo de ajuste se completará después de que se hayan completado dos series de tres trabajos en paralelo.  Para la estrategia de ajuste de Optimización Bayesiana por defecto utilizada aquí, la búsqueda de ajuste se basa en los resultados de grupos anteriores de trabajos de entrenamiento, por lo que no ejecutamos todos los trabajos en paralelo, sino que dividimos los trabajos en grupos de trabajos paralelos.\n",
    "\n",
    "Hay una contrapartida: si se utilizan más trabajos paralelos se terminará el tuning más pronto, pero probablemente se sacrificará el accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos lanzar un trabajo de ajuste de hiperparámetros llamando al método `fit` del objeto `HyperparameterTuner`. El trabajo de ajuste puede tardar unos 10 minutos en terminar.  Mientras espera, el estado del job, incluyendo los metadatos y los resultados de los trabajos de entrenamiento individuales dentro del trabajo de ajuste, se pueden revisar en la consola de SageMaker en el panel **Trabajos de ajuste de hiperparámetros**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(\n",
    "  hosted_estimator,\n",
    "  objective_metric_name,\n",
    "  hyperparameter_ranges,\n",
    "  metric_definitions,\n",
    "  max_jobs=6,\n",
    "  max_parallel_jobs=3,\n",
    "  objective_type=objective_type)\n",
    "\n",
    "tuning_job_name = f'tf-2-workflow-{strftime(\"%d-%H-%M-%S\", gmtime())}'\n",
    "tuner.fit(inputs, job_name=tuning_job_name)\n",
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Una vez finalizado el trabajo de ajuste, podemos utilizar el objeto `HyperparameterTuningJobAnalytics` del SDK SageMaker Python para listar los 5 trabajos de ajuste con mejor rendimiento. Aunque los resultados varían de un trabajo de ajuste a otro, el mejor validation loss del job (columna `FinalObjectiveValue`) probablemente será sustancialmente menor que el validation loss del trabajo de entrenamiento que hicimos anteriormente, sin ningún ajuste aparte de aumentar manualmente el número de epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tuner_metrics = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name)\n",
    "tuner_metrics.dataframe().sort_values(['FinalObjectiveValue'], ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "El tiempo total de entrenamiento y el estado de los trabajos pueden comprobarse con las siguientes líneas de código.\n",
    "\n",
    "Dado que la parada automática anticipada está desactivada por defecto, todos los trabajos de entrenamiento deberían completarse con normalidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "total_time = tuner_metrics.dataframe()['TrainingElapsedTimeSeconds'].sum() / 3600\n",
    "print(\"The total training time is {:.2f} hours\".format(total_time))\n",
    "tuner_metrics.dataframe()['TrainingJobStatus'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker hosted endpoint <a class=\"anchor\" id=\"SageMakerHostedEndpoint\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Asumiendo que el mejor modelo del trabajo de ajuste es mejor que el modelo producido por el trabajo de entrenamiento del notebook anterior, ahora podríamos desplegar fácilmente ese modelo en producción. \n",
    "\n",
    "Una opción conveniente es usar un endpoint alojado en SageMaker, que sirve predicciones en tiempo real del modelo entrenado (Para predicciones asíncronas y offline en grandes Datasets, podríamos usar SageMaker Processing o SageMaker Batch Transform). El endpoint recuperará TensorFlow SavedModel creado durante el entrenamiento y lo desplegará dentro de un contenedor SageMaker TensorFlow Serving. Todo con una línea de código.  \n",
    "\n",
    "Más específicamente, llamando al método `deploy` del objeto `HyperparameterTuner` que instanciamos arriba, podemos desplegar directamente el mejor modelo del trabajo de ajuste a un endpoint alojado en SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tuning_predictor = tuner.deploy(initial_instance_count=1, instance_type='ml.c5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Podemos comparar las predicciones generadas por el endpoint con los valores target reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = tuning_predictor.predict(x_test[:10])['predictions'] \n",
    "flat_list = [float('%.1f'%(item)) for sublist in results for item in sublist]\n",
    "print(f'predictions: \\t{np.array(flat_list)}')\n",
    "print(f'target values: \\t{y_test[:10].round(decimals=1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Para evitar cargos de facturación por recursos no utilizados, podemos eliminar el endpoint de predicción para liberar las instancias asociadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(tuning_predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Scoring <a class=\"anchor\" id=\"BatchScoringStep\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "El último paso de este proceso es la puntuación por lotes (inferencia/predicción). Los inputs de este paso serán el modelo que hemos entrenado anteriormente y los datos de prueba. Solamente necesitamos un sencillo script de Python para realizar la inferencia por lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile batch-score.py\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import tarfile\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    install('tensorflow==2.3.1')\n",
    "    model_path = f\"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path, 'r:gz') as tar:\n",
    "      tar.extractall('./model')\n",
    "    import tensorflow as tf\n",
    "    model = tf.keras.models.load_model('./model/1')\n",
    "    test_path = \"/opt/ml/processing/test/\"\n",
    "    x_test = np.load(os.path.join(test_path, 'x_test.npy'))\n",
    "    y_test = np.load(os.path.join(test_path, 'y_test.npy'))\n",
    "    scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "    print(\"\\nTest MSE :\", scores)\n",
    "    \n",
    "    output_dir = \"/opt/ml/processing/batch\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    evaluation_path = f\"{output_dir}/score-report.txt\"\n",
    "    with open(evaluation_path, 'w') as writer:\n",
    "      writer.write(f\"Test MSE : {scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Aquí utilizaremos SageMaker Processing para realizar el scoring por lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "framework_version = \"0.23-1\"\n",
    "batch_instance_type = \"ml.t3.medium\"\n",
    "batch_instance_count = 1\n",
    "batch_scorer = SKLearnProcessor(\n",
    "  framework_version=\"0.23-1\",\n",
    "  instance_type=\"ml.c5.xlarge\",\n",
    "  instance_count=1,\n",
    "  base_job_name=\"tf-2-workflow-batch\",\n",
    "  role=execution_role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_scorer.run(\n",
    "  inputs=[\n",
    "    ProcessingInput(\n",
    "      source=tuner.best_estimator().model_data,\n",
    "      destination=\"/opt/ml/processing/model\"\n",
    "    ),\n",
    "    ProcessingInput(\n",
    "      source=sklearn_processor1.latest_job.outputs[1].destination,    # [0] es train, [1] es test\n",
    "      destination=\"/opt/ml/processing/test\"\n",
    "    )\n",
    "  ],\n",
    "  outputs=[\n",
    "    ProcessingOutput(output_name=\"batch\",\n",
    "    source=\"/opt/ml/processing/batch\")\n",
    "  ],\n",
    "  code=\"./batch-score.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "report_path = f\"{batch_scorer.latest_job.outputs[0].destination}/score-report.txt\"\n",
    "!aws s3 cp {report_path} ./score-report.txt --quiet && cat score-report.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.3-cpu-py37-ubuntu18.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

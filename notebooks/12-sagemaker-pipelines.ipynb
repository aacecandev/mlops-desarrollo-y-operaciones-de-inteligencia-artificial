{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatizando la preparación del Dataset y el entrenamiento del modelo con SageMaker Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Vamos a crear un flujo de trabajo de producción repetible que normalmente se ejecuta fuera de los notebooks. Para demostrar la automatización del workflow, utilizaremos [Amazon SageMaker Pipelines](https://aws.amazon.com/sagemaker/pipelines) para la orquestación. SageMaker Pipelines nos ayuda a automatizar los diferentes pasos del workflow de ML, incluyendo el procesamiento de datos, el entrenamiento del modelo y la predicción por lotes (scoring), y a aplicar condiciones como aprobación de la calidad del modelo. También incluye un registro de modelos y un rastreador de linaje de los modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow automatizado con SageMaker Pipelines <a class=\"anchor\" id=\"WorkflowAutomation\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "En los pasos anteriores hemos prototipado varios pasos de un proyecto TensorFlow dentro del propio notebook, con algunos pasos ejecutados en recursos externos de SageMaker (entrenamiento alojado, ajuste de modelos, endpoints alojados). Los notebook son excelentes para la creación de prototipos, pero por lo general no se utilizan en la producción de pipelines de aprendizaje automático.  \n",
    "\n",
    "Un pipeline sencillo en SageMaker incluye el procesamiento del Dataset para prepararlo para el entrenamiento, el propio entrenamiento y, a continuación, el uso del modelo para realizar algún tipo de inferencia, como predicción por lotes (scoring). Utilizaremos SageMaker Pipelines para automatizar estos pasos, manteniendo el pipeline lo más sencillo posible por ahora: se puede extender a un pipeline mucho más complejo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parámetros del Pipeline <a class=\"anchor\" id=\"PipelineParameters\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Antes de empezar a crear el pipeline en sí, deberíamos pensar en cómo parametrizarlo. Por ejemplo, podemos utilizar diferentes tipos de instancia para diferentes propósitos, como tipos basados en CPU para el procesamiento de datos y tipos basados en GPU o más potentes para el entrenamiento del modelo.  Todos estos son \"nudos\" del pipeline que podemos parametrizar. La parametrización permite realizar ejecuciones y programaciones personalizadas sin tener que modificar la definición del pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade sagemaker=='v2.90.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.session.Session()\n",
    "bucket = sess.default_bucket() \n",
    "\n",
    "raw_s3 = f\"s3://{bucket}/tf-2-workflow/data/raw\"    # sess.upload_data(path='./data/raw/', key_prefix=rawdata_s3_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "# raw input data\n",
    "input_data = ParameterString(name=\"InputData\", default_value=raw_s3)\n",
    "\n",
    "# processing step parameters\n",
    "processing_instance_type = ParameterString(name=\"ProcessingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "\n",
    "# training step parameters\n",
    "training_instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.c5.2xlarge\")\n",
    "training_instance_count = ParameterInteger(name=\"TrainingInstanceCount\", default_value=1)\n",
    "\n",
    "# batch inference step parameters\n",
    "batch_instance_type = ParameterString(name=\"BatchInstanceType\", default_value=\"ml.c5.xlarge\")\n",
    "batch_instance_count = ParameterInteger(name=\"BatchInstanceCount\", default_value=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step de preprocesado <a class=\"anchor\" id=\"ProcessingStep\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "El primer paso en el pipeline será el preprocesamiento de los datos para prepararlos para el entrenamiento. Creamos un objeto `SKLearnProcessor` similar al visto anteriormente, pero ahora parametrizado para que podamos hacer un seguimiento por separado y cambiar la configuración del job según sea necesario, por ejemplo, para aumentar el tamaño del tipo de instancia para trabajar con un Dataset que va creciendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    input_files = glob.glob('{}/*.npy'.format('/opt/ml/processing/input'))\n",
    "    print('\\nINPUT FILE LIST: \\n{}\\n'.format(input_files))\n",
    "    scaler = StandardScaler()\n",
    "    x_train = np.load(os.path.join('/opt/ml/processing/input', 'x_train.npy'))\n",
    "    scaler.fit(x_train)\n",
    "    for file in input_files:\n",
    "        raw = np.load(file)\n",
    "        # Solo transformamos las columnas con features\n",
    "        if 'y_' not in file:\n",
    "            transformed = scaler.transform(raw)\n",
    "        if 'train' in file:\n",
    "            if 'y_' in file:\n",
    "                output_path = os.path.join('/opt/ml/processing/train', 'y_train.npy')\n",
    "                np.save(output_path, raw)\n",
    "                print('SAVED LABEL TRAINING DATA FILE\\n')\n",
    "            else:\n",
    "                output_path = os.path.join('/opt/ml/processing/train', 'x_train.npy')\n",
    "                np.save(output_path, transformed)\n",
    "                print('SAVED TRANSFORMED TRAINING DATA FILE\\n')\n",
    "        else:\n",
    "            if 'y_' in file:\n",
    "                output_path = os.path.join('/opt/ml/processing/test', 'y_test.npy')\n",
    "                np.save(output_path, raw)\n",
    "                print('SAVED LABEL TEST DATA FILE\\n')\n",
    "            else:\n",
    "                output_path = os.path.join('/opt/ml/processing/test', 'x_test.npy')\n",
    "                np.save(output_path, transformed)\n",
    "                print('SAVED TRANSFORMED TEST DATA FILE\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "sess = sagemaker.session.Session()\n",
    "bucket = sess.default_bucket() \n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"tf-2-workflow-process\",\n",
    "    sagemaker_session=sess,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"TF2Process\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\", s3_data_distribution_type='ShardedByS3Key'),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    code=\"./preprocessing.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps de entrenamiento y creación de modelo <a class=\"anchor\" id=\"TrainingModelCreation\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "El siguiente código configura un setp del pipeline para realizar el entrenamiento. Comenzamos especificando qué contenedor de entrenamiento TensorFlow 2 preconstruido por SageMaker se utilizará para el trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "tensorflow_version = '2.3.1'\n",
    "python_version = 'py37'\n",
    "\n",
    "image_uri_train = sagemaker.image_uris.retrieve(\n",
    "  framework=\"tensorflow\",\n",
    "  region=region,\n",
    "  version=tensorflow_version,\n",
    "  py_version=python_version,\n",
    "  instance_type=training_instance_type,\n",
    "  image_scope=\"training\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A continuación, especificamos un objeto `Estimator`, y definimos un `TrainingStep` para insertar el job de entrenamiento en el pipeline recibiendo los inputs del paso anterior de preprocesamiento. Nótese que hemos utilizado los hiperparámetros del mejor estimador que encontramos en el notebook. La integración de AutoTuning con los Pipelines de SageMaker ya está disponible, pero aún no ha sido integrada en este notebook (se actualizará pronto). \n",
    "\n",
    "Deberíamos usarlo para encontrar el mejor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model_path = f\"s3://{bucket}/TF2WorkflowTrain\"\n",
    "training_parameters = {'epochs': 21, 'batch_size': 247, 'learning_rate': 0.138448, 'for_pipeline': 'true'}\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    image_uri=image_uri_train,\n",
    "    source_dir='code',\n",
    "    entry_point='train.py',\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=training_instance_count,\n",
    "    role=role,\n",
    "    base_job_name=\"tf-2-workflow-train\",\n",
    "    output_path=model_path,\n",
    "    hyperparameters=training_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "step_train = TrainingStep(\n",
    "    name=\"TF2WorkflowTrain\",\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri\n",
    "        ),\n",
    "        \"test\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Como paso adicional, creamos un objeto `Model` para envolver el artefacto del modelo, y lo asociamos con un contenedor de inferencia TensorFlow pre-construido por SageMaker para usarlo potencialmente más tarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "image_uri_inference = sagemaker.image_uris.retrieve(\n",
    "    framework=\"tensorflow\",\n",
    "    region=region,\n",
    "    version=tensorflow_version,\n",
    "    py_version=python_version,\n",
    "    instance_type=batch_instance_type,\n",
    "    image_scope=\"inference\"\n",
    ")\n",
    "\n",
    "model = Model(\n",
    "    image_uri=image_uri_inference,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sess,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "inputs_model = CreateModelInput(\n",
    "    instance_type=batch_instance_type\n",
    ")\n",
    "\n",
    "step_create_model = CreateModelStep(\n",
    "    name=\"TF2WorkflowCreateModel\",\n",
    "    model=model,\n",
    "    inputs=inputs_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso de puntuación por lotes <a class=\"anchor\" id=\"BatchScoringStep\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "El último paso de este proceso es la puntuación por lotes (inferencia/predicción). El inputde este paso serán el modelo que hemos entrenado anteriormente y los datos de prueba. Para realizar la inferencia por lotes solamente necesitamos un sencillo script de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%%writefile batch-score.py\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import tarfile\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    install('tensorflow==2.3.1')\n",
    "    model_path = f\"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path, 'r:gz') as tar:\n",
    "        tar.extractall('./model')\n",
    "    import tensorflow as tf\n",
    "    model = tf.keras.models.load_model('./model/1')\n",
    "    test_path = \"/opt/ml/processing/test/\"\n",
    "    x_test = np.load(os.path.join(test_path, 'x_test.npy'))\n",
    "    y_test = np.load(os.path.join(test_path, 'y_test.npy'))\n",
    "    scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "    print(\"\\nTest MSE :\", scores)\n",
    "    \n",
    "    output_dir = \"/opt/ml/processing/batch\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    evaluation_path = f\"{output_dir}/score-report.txt\"\n",
    "    with open(evaluation_path, 'w') as writer:\n",
    "        writer.write(f\"Test MSE : {scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Aquí utilizaremos SageMaker Processing para realizar la puntuación por lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "batch_scorer = SKLearnProcessor(\n",
    "                    framework_version=framework_version,\n",
    "                    instance_type=batch_instance_type,\n",
    "                    instance_count=batch_instance_count,\n",
    "                    base_job_name=\"tf-2-workflow-batch\",\n",
    "                    sagemaker_session=sess,\n",
    "                    role=role )\n",
    "\n",
    "step_batch = ProcessingStep(\n",
    "                    name=\"TF2WorkflowBatchScoring\",\n",
    "                    processor=batch_scorer,\n",
    "                    inputs=[\n",
    "                        ProcessingInput(\n",
    "                            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "                            destination=\"/opt/ml/processing/model\"\n",
    "                        ),\n",
    "                        ProcessingInput(\n",
    "                            source=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                                \"test\"\n",
    "                            ].S3Output.S3Uri,\n",
    "                            destination=\"/opt/ml/processing/test\"\n",
    "                        )\n",
    "                    ],\n",
    "                    outputs=[\n",
    "                        ProcessingOutput(output_name=\"batch\", source=\"/opt/ml/processing/batch\"),\n",
    "                    ],\n",
    "                    code=\"./batch-score.py\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creando y ejecutando el pipeline <a class=\"anchor\" id=\"CreatingExecutingPipeline\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Una vez definidos todos los pasos del pipeline, podemos definir el propio pipeline como un objeto `Pipeline` que comprende una serie de esos pasos.  También son posibles los pasos paralelos y condicionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"TF2Workflow\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        input_data,\n",
    "        processing_instance_type, \n",
    "        processing_instance_count, \n",
    "        training_instance_type, \n",
    "        training_instance_count,\n",
    "        batch_instance_type,\n",
    "        batch_instance_count\n",
    "    ],\n",
    "    steps=[step_process, \n",
    "        step_train, \n",
    "        step_create_model,\n",
    "        step_batch\n",
    "    ],\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Podemos inspeccionar la definición del pipeline en formato JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "After upserting its definition, we can start the pipeline with the `Pipeline` object's `start` method:\n",
    "\n",
    "Cuando estemos de acuerdo con la definición producida, podemos iniciar el pipeline con el método `start` del objeto `Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Ahora podemos confirmar que el pipeline se está ejecutando. En la salida del log de la siguiente celda confirmamos que `PipelineExecutionStatus` es igual a `Executing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revisar el Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the pipeline started executing, you can view the pipeline run. \n",
    "\n",
    "To view them, choose the SageMakers Components and registries button.\n",
    "On the Components and registires drop down, select Pipelines.\n",
    "\n",
    "Una vez que el pipeline ha comenzado a ejecutarse, podemos ver su ejecución en la consola Web.\n",
    "\n",
    "Para verlo, seleccionamos el botón Componentes y registros de SageMaker. En el menú desplegable Componentes y registros, seleccionamos Pipelines.\n",
    "\n",
    "![sageMakers_components_and_registries_button](../media/img/pipelines_execution_1.png)\n",
    "\n",
    "Hacemos click en el pipeline `TF2Workflow`, y después doble click sobre su ejecución.\n",
    "\n",
    "![click_the_pipeline_execution](../media/img/pipelines_execution_2.png)\n",
    "\n",
    "Ahora podemos ver el pipeline ejecutándose. Podemos hacer click en el step `TF2Process` para ver detalles adicionales.\n",
    "\n",
    "![view_pipeline_execution](../media/img/pipelines_execution_3.png)\n",
    "\n",
    "En este step específico, podremos ver el output, logs e información adicional.\n",
    "\n",
    "![view_step_details](../media/img/pipelines_execution_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Normalmente, este proceso debería tardar unos 10 minutos en completarse. Podemos esperar a que se complete invocando `wait()`. Una vez completada la ejecución, podemos listar el estado de los pasos del pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "execution.wait()\n",
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprobación del informe de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Una vez completado el trabajo de puntuación por lotes en el pipeline, éste se sube a S3. Por simplicidad este informe simplemente indica el MSE de la prueba, pero generalmente estos informes pueden incluir tantos detalles como se desee. También pueden ser formateados para su uso en pasos de aprobación condicional en pipelines de SageMaker. Por ejemplo, el pipeline podría tener un paso condicional que sólo permite continuar si el MSE es inferior a un umbral determinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "report_path = f\"{step_batch.outputs[0].destination}/score-report.txt\"\n",
    "!aws s3 cp {report_path} ./score-report.txt --quiet && cat score-report.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seguimiento del linaje  <a class=\"anchor\" id=\"LineageOfPipelineArtifacts\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "El seguimiento del linaje de SageMaker crea y almacena información sobre los pasos de un flujo de trabajo de ML desde la preparación de los datos hasta el despliegue del modelo. Con la información de seguimiento podemos reproducir los pasos del flujo de trabajo, realizar un seguimiento del linaje del modelo y del Dataset y establecer estándares de auditoría y gobernanza del modelo.\n",
    "\n",
    "Comprobemos ahora el linaje del modelo generado por el pipeline anterior. La tabla de linaje identifica los recursos utilizados en el entrenamiento, incluyendo las fuentes de datos de entrenamiento y de prueba con marca de tiempo, y la versión específica del contenedor TensorFlow 2 en uso durante el trabajo de entrenamiento.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
    "\n",
    "for execution_step in reversed(execution.list_steps()):\n",
    "    if execution_step['StepName'] == 'TF2WorkflowTrain':\n",
    "        display(viz.show(pipeline_execution_step=execution_step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensiones <a class=\"anchor\" id=\"Extensions\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We've covered a lot of content in this notebook:  SageMaker Processing for data transformation, Automatic Model Tuning, and SageMaker hosted training and inference.  These are central elements for most deep learning workflows in SageMaker.  Additionally, we examined how SageMaker Pipelines helps automate deep learning workflows after completion of the prototyping phase of a project.\n",
    "\n",
    "Besides all of the SageMaker features explored above, there are many other features that may be applicable to your project.  For example, to handle common problems during deep learning model training such as vanishing or exploding gradients, **SageMaker Debugger** is useful.  To manage common problems such as data drift after a model is in production, **SageMaker Model Monitor** can be applied.\n",
    "\n",
    "Hemos cubierto mucho contenido en este notebook:\n",
    "\n",
    "Procesamiento de SageMaker para la transformación de datos, ajuste automático de modelos y entrenamiento e inferencia alojados en SageMaker. Estos elementos son centrales para la mayoría de los workflows de ML en SageMaker.\n",
    "\n",
    "Además, hemos examinado como automatizar dichos workflows con SageMaker Pipelines tras completar la fase de creación de prototipos de un proyecto.\n",
    "\n",
    "Además de todas las características de SageMaker anteriores, hay muchas otras características que pueden ser aplicables a nuestros proyecto. Por ejemplo, para solventar problemas comunes durante el entrenamiento del modelo, como problemas con gradientes, es útil **SageMaker Debugger**. Para gestionar problemas como el data drift tras poner un modelo en producción podemos usar **SageMaker Model Monitor**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

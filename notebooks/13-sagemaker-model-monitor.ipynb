{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitorización del modeloModel Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este cuaderno vamos a aprender lo siguiente:\n",
    "\n",
    "* Alojar un modelo en Amazon SageMaker y capturar solicitudes de inferencia, resultados y metadatos.\n",
    "* Analizar un Dataset de entrenamiento para generar restricciones de referencia\n",
    "* Supervisar un endpoint desplegado para detectar violaciones de las restricciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Setup\n",
    "\n",
    "Para empezar, hemos de que ha completado estos requisitos previos.\n",
    "\n",
    "* Especificar una región de AWS para alojar el modelo.\n",
    "* Disponer del ARN de un rol IAM que de acceso a Amazon SageMaker a los datos en Amazon Simple Storage Service (Amazon S3).\n",
    "* Tener creado un bucket de S3 para almacenar los datos utilizados para entrenar su modelo, cualquier dato adicional del modelo y los datos capturados de las invocaciones del modelo. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.session.Session()\n",
    "bucket = sess.default_bucket() \n",
    "prefix = 'tf-2-workflow'\n",
    "\n",
    "s3_capture_upload_path = f's3://{bucket}/{prefix}/monitoring/datacapture'\n",
    "\n",
    "reports_prefix = f'{prefix}/reports'\n",
    "s3_report_path = f's3://{bucket}/{reports_prefix}'\n",
    "\n",
    "print(f\"Capture path: {s3_capture_upload_path}\")\n",
    "print(f\"Report path: {s3_report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTE A: Capturando datos de inferencia en tiempo real Capturing real-time inference data from Amazon SageMaker endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear un endpoint para mostrar la capacidad de captura de datos en acción.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Despliegue del modelo en Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a comenzar desplegando el modelo entrenado en `notebooks/10-sagemaker-using-sagemaker-apis.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def get_latest_training_job_name(base_job_name):\n",
    "    client = boto3.client('sagemaker')\n",
    "    response = client.list_training_jobs(\n",
    "        NameContains=base_job_name, SortBy='CreationTime', \n",
    "        SortOrder='Descending', StatusEquals='Completed'\n",
    "    )\n",
    "    \n",
    "    if len(response['TrainingJobSummaries']) > 0 :\n",
    "        return response['TrainingJobSummaries'][0]['TrainingJobName']\n",
    "    \n",
    "    else:\n",
    "        raise Exception('Training job not found.')\n",
    "\n",
    "def get_training_job_s3_model_artifacts(job_name):\n",
    "    client = boto3.client('sagemaker')\n",
    "    response = client.describe_training_job(TrainingJobName=job_name)\n",
    "    s3_model_artifacts = response['ModelArtifacts']['S3ModelArtifacts']\n",
    "    return s3_model_artifacts\n",
    "\n",
    "latest_training_job_name = get_latest_training_job_name('tf-2-workflow')\n",
    "print(latest_training_job_name)\n",
    "model_path = get_training_job_s3_model_artifacts(latest_training_job_name)\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí creamos el objeto de tipo modelo con la imagen y el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "\n",
    "tensorflow_model = TensorFlowModel(\n",
    "    model_data = model_path,\n",
    "    role = role,\n",
    "    framework_version = '2.3.1'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activamos la captura de datos al desplegar el endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "endpoint_name = 'tf-2-workflow-endpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "\n",
    "predictor = tensorflow_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    endpoint_name=endpoint_name,\n",
    "    # Activamos la captura de datos\n",
    "    data_capture_config=DataCaptureConfig(\n",
    "        enable_capture=True,\n",
    "        sampling_percentage=100,\n",
    "        destination_s3_uri=s3_capture_upload_path\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación del Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, importamos el Dataset. El Dataset en sí es pequeño y está relativamente libre de problemas. Por ejemplo, no hay valores nulos, un problema bastante común. Por tanto, el preprocesamiento sólo consta de la normalización de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.python.keras.datasets import boston_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invocación del modelo desplegado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos enviar datos a este endpoint para obtener inferencias en tiempo real. Dado que hemos habilitado la captura de datos en los pasos anteriores, el payload de la solicitud y la respuesta, junto con algunos metadatos adicionales, se guardan en la ubicación de S3 que hemos especificado en `DataCaptureConfig`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step invokes the endpoint with included sample data for about 3 minutes. Data is captured based on the sampling percentage specified and the capture continues until the data capture option is turned off.\n",
    "\n",
    "Este paso invoca al endpoint con datos de muestra durante unos 3 minutos. Los datos se capturan en función del porcentaje de muestreo especificado y la captura continúa hasta que se desactiva la opción de captura de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "import time\n",
    "\n",
    "print(f\"Sending test traffic to the endpoint {endpoint_name}. \\nPlease wait...\")\n",
    "\n",
    "flat_list =[]\n",
    "for item in x_test:\n",
    "    result = predictor.predict(item)['predictions'] \n",
    "    flat_list.append(float('%.1f'%(np.array(result))))\n",
    "    time.sleep(1.8)\n",
    "    \n",
    "print(\"Done!\")\n",
    "print(f'predictions: \\t{np.array(flat_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de los datos capturados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se enumeran los archivos de captura de datos almacenados en Amazon S3. Deberíamos ver diferentes archivos de diferentes períodos de tiempo organizados en base a la hora en la que se produjo la invocación. El formato de la ruta de Amazon S3 es `s3://{destination-bucket-prefix}/{endpoint-name}/{variant-name}/yyyy/mm/dd/hh/filename.jsonl`.\n",
    "\n",
    "<b>La llegada de los datos capturados a Amazon S3 puede tardar un par de minutos, por lo que la siguiente celda podría dar error. Si esto pasa, reintentaremos después de un minuto.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client('s3')\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix='tf-2-workflow/monitoring/datacapture/')\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get('Contents')]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, visualizamos el contenido de un solo archivo de captura. Podemos ver todos los datos capturados en un archivo con formato JSON específico de Amazon SageMaker. Echemos un vistazo a las primeras líneas del archivo capturado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get('Body').read().decode(\"utf-8\")\n",
    "\n",
    "capture_file = get_obj_body(capture_files[-1])\n",
    "print(capture_file[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último veamos el contenido de una sola línea en un archivo JSON formateado para que se apreciar observarlo un poco mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "print(json.dumps(json.loads(capture_file.split('\\n')[0]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puedmos ver, cada solicitud de inferencia se captura en una línea en el archivo JSON. La línea contiene tanto la entrada como la salida combinadas. En el ejemplo proporcionamos el ContentType como `text/csv` que se refleja en el valor `observedContentType`. Además, expusimos el tipo de codificación utilizado para codificar los payloads de entrada y salida en el formato de captura con el valor `encoding`.\n",
    "\n",
    "Para recapitular, hemos observado cómo habilitar la captura de payloads de entrada o salida a un endpoint con un nuevo parámetro. También hemos observado el aspecto del formato de captura en Amazon S3.\n",
    "\n",
    "A continuación, vamos a tratar de monitorizar los datos recopilados en Amazon S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTE B: Monitorización del modelo. Estableciendo lineas base y monitorización continua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además de recopilar los datos, Amazon SageMaker proporciona la capacidad de supervisar y evaluar los datos observados por los endpoints. Para ello:\n",
    "\n",
    "1. Creamos una línea de base con la que comparar el tráfico en tiempo real.\n",
    "1. Una vez que la línea de base está lista, configuramos un schedule para evaluar y comparar continuamente con la línea de base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sugerencia de restricción con baseline/training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Dataset de entrenamiento con el que se ha entrenado el modelo suele ser una buena referencia. Teniendo en mente que el esquema de los datos del Dataset de entrenamiento y el de inferencia deben coincidir exactamente (es decir, el número y el orden de las features).\n",
    "\n",
    "A partir del Dataset de entrenamiento podemos pedir a SageMaker que sugiera un conjunto de \"restricciones\" de referencia y genere \"estadísticas\" descriptivas para explorar los datos. Para este ejemplo, subiremos el Dataset de entrenamiento que se utilizó para entrenar el modelo preentrenado. Si ya lo tienes en Amazon S3, puedes apuntar directamente a él."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación del Dataset de entrenamiento con cabeceras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dt = pd.DataFrame(\n",
    "  data = x_train, \n",
    "  columns = [\n",
    "    \"CRIM\",\n",
    "    \"ZN\",\n",
    "    \"INDUS\",\n",
    "    \"CHAS\",\n",
    "    \"NOX\",\n",
    "    \"RM\",\n",
    "    \"AGE\",\n",
    "    \"DIS\",\n",
    "    \"RAD\",\n",
    "    \"TAX\",\n",
    "    \"PTRATIO\",\n",
    "    \"B\",\n",
    "    \"LSTAT\"\n",
    "  ]\n",
    ")\n",
    "\n",
    "dt.to_csv(\"training-dataset-with-header.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copiamos el conjunto de datos de entrenamiento en Amazon S3 (si ya lo tienes en Amazon S3, puedes reutilizarlo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_prefix = prefix + '/baselining'\n",
    "baseline_data_prefix = baseline_prefix + '/data'\n",
    "baseline_results_prefix = baseline_prefix + '/results'\n",
    "\n",
    "baseline_data_uri = f's3://{bucket}/{baseline_data_prefix}'\n",
    "baseline_results_uri = f's3://{bucket}/{baseline_results_prefix}'\n",
    "print(f'Baseline data uri: {baseline_data_uri}')\n",
    "print(f'Baseline results uri: {baseline_results_uri}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_file = open(\"training-dataset-with-header.csv\", 'rb')\n",
    "s3_key = os.path.join(baseline_prefix, 'data', 'training-dataset-with-header.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(s3_key).upload_fileobj(training_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear un job para establecer una línea de referencia con el Dataset de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the training data ready in Amazon S3, start a job to `suggest` constraints. `DefaultModelMonitor.suggest_baseline(..)` starts a `ProcessingJob` using an Amazon SageMaker provided Model Monitor container to generate the constraints.\n",
    "\n",
    "Ahora que tenemos los datos de entrenamiento listos en S3, iniciamos un job para `sugerir` restricciones. `DefaultModelMonitor.suggest_baseline(..)` inicia un `ProcessingJob` utilizando un contenedor de Model Monitor proporcionado por SageMaker para generar las restricciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_data_uri+'/training-dataset-with-header.csv',\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploración de las restricciones generadas y las estadísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client('s3')\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=baseline_results_prefix)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get('Contents')]\n",
    "print(\"Found Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.io.json.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_df = pd.io.json.json_normalize(baseline_job.suggested_constraints().body_dict[\"features\"])\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analizando los datos recolectados para problemas de calidad en los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de un schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos crear un schedule de supervisión del modelo para el endpoint creado anteriormente. Utilizaremos los recursos de referencia (restricciones y estadísticas) para comparar con el tráfico en tiempo real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el análisis anterior, hemos visto cómo se guardan los datos capturados - ese es el formato estándar de entrada y salida para los modelos Tensorflow. Pero Model Monitor es agnóstico al framework, y espera un formato [explicado en la documentación](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-pre-and-post-processing.html#model-monitor-pre-processing-script):\n",
    "\n",
    "```json\n",
    "- Input\n",
    "    - Flattened JSON `{\"feature0\": <value>, \"feature1\": <value>...}`\n",
    "    - Tabular `\"<value>, <value>...\"`\n",
    "- Output:\n",
    "    - Flattened JSON `{\"prediction0\": <value>, \"prediction1\": <value>...}`\n",
    "    - Tabular `\"<value>, <value>...\"`\n",
    "```\n",
    "\n",
    "Necesitamos transformar los registros de entrada para cumplir con este requisito. Model Monitor ofrece _scripts de preprocesamiento_ en Python para transformar la entrada. La celda de abajo tiene el script que funcionará para nuestro caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import json\n",
    "\n",
    "def preprocess_handler(inference_record):\n",
    "    input_data = json.loads(inference_record.endpoint_input.data)\n",
    "    input_data = {f\"feature{i}\": val for i, val in enumerate(input_data)}\n",
    "    \n",
    "    output_data = json.loads(inference_record.endpoint_output.data)[\"predictions\"][0][0]\n",
    "    output_data = {\"prediction0\": output_data}\n",
    "    \n",
    "    return{**input_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subiremos este script a un s3 y lo pasaremos como el parámetro `record_preprocessor_script` de la llamada `create_monitoring_schedule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_s3_dest_path = f\"s3://{bucket}/{prefix}/artifacts/modelmonitor\"\n",
    "preprocessor_s3_dest = sagemaker.s3.S3Uploader.upload(\"preprocessing.py\", preprocessor_s3_dest_path)\n",
    "print(preprocessor_s3_dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from time import gmtime, strftime\n",
    "\n",
    "mon_schedule_name = 'DEMO-tf-2-workflow-model-monitor-schedule-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    endpoint_input=predictor.endpoint,\n",
    "    record_preprocessor_script=preprocessor_s3_dest,\n",
    "    output_s3_uri=s3_report_path,\n",
    "    statistics=my_default_monitor.baseline_statistics(),\n",
    "    constraints=my_default_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generando violaciones de las restricciones artificialmente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener algún resultado relevante para el análisis de la monitorización, podemos  generar artificialmente algunas inferencias con valores de features que causen violaciones específicas, y luego invocar el endpoint con estos datos.\n",
    "\n",
    "Mirando nuestras características RM y AGE:\n",
    "\n",
    "- RM - número medio de habitaciones por vivienda\n",
    "- AGE - proporción de viviendas ocupadas por sus propietarios construidas antes de 1940\n",
    "\n",
    "Vamos a simular una situación en la que el número medio de habitaciones y la proporción de viviendas ocupadas por sus propietarios son ambos -10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_violations = pd.read_csv(\"training-dataset-with-header.csv\")\n",
    "df_with_violations[\"RM\"] = -10\n",
    "df_with_violations[\"AGE\"] = -10\n",
    "df_with_violations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar tráfico artificial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La celda de abajo inicia un hilo para enviar tráfico al endpoint. Hay que tener en cuenta  que necesitaremos detener el kernel para terminar este hilo. Si no hay tráfico, los trabajos de monitorización se marcan como `Failed` ya que no hay datos que procesar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from time import sleep\n",
    "import time\n",
    "\n",
    "def invoke_endpoint():\n",
    "    for item in df_with_violations.to_numpy():\n",
    "        result = predictor.predict(item)['predictions'] \n",
    "        time.sleep(0.5)\n",
    "\n",
    "def invoke_endpoint_forever():\n",
    "    while True:\n",
    "        invoke_endpoint()\n",
    "        \n",
    "thread = Thread(target = invoke_endpoint_forever)\n",
    "thread.start()\n",
    "\n",
    "# Hay que parar el kernel para detener las invocaciones al endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describir e inspeccionar el schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que describimos, observemos que `MonitoringScheduleStatus` cambia a `Scheduled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_schedule_result = my_default_monitor.describe_schedule()\n",
    "print(f\"Schedule status: {desc_schedule_result['MonitoringScheduleStatus']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listar ejecuciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El schedule inicia los jobs en los intervalos previamente especificados. Vamos a enumerar las últimas cinco ejecuciones. Hay que tener en cuenta que si lanzamos esto después de crear el schedule, las ejecuciones podrían ser nulas. Es posible que tengamos que esperar a cruzar el límite de la hora (en UTC) para ver las ejecuciones iniciadas. El código de abajo implementa la lógica para esperar.\n",
    "\n",
    "Nota: Incluso para una programación horaria, SageMaker tiene un periodo de buffer de 20 minutos para programar su ejecución. Podemos comprobar ver que nuestra ejecución comienza en cualquier momento entre cero y ~20 minutos desde el límite de la hora. Esto es controlado y se hace para equilibrar la carga en el backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_executions = my_default_monitor.list_executions()\n",
    "print(\"We created a hourly schedule above and it will kick off executions ON the hour (plus 0 - 20 min buffer.\\nWe will have to wait till we hit the hour...\")\n",
    "\n",
    "while len(mon_executions) == 0:\n",
    "    print(\"Waiting for the 1st execution to happen...\")\n",
    "    time.sleep(60)\n",
    "    mon_executions = my_default_monitor.list_executions()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspección de una ejecución específica (última)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En la celda anterior, se recogió la última ejecución programada completada o fallida. Aquí están los posibles estados de finalización y lo que significa cada uno de ellos:\n",
    "\n",
    "* Completed - Esto significa que la ejecución de monitoreo se completó y no se encontraron problemas en el informe de violaciones.\n",
    "* CompletedWithViolations - Esto significa que la ejecución se completó, pero se detectaron violaciones de las restricciones.\n",
    "* Failed - La ejecución de monitoreo falló, tal vez debido a un error del cliente (tal vez permisos incorrectas del rol ) o problemas de infraestructura. Es necesario un examen más detallado de `FailureReason` y `ExitMessage` para identificar qué ocurrió exactamente.\n",
    "* Stopped: el trabajo superó el tiempo máximo de ejecución o se detuvo manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_execution = mon_executions[-1] # el index de la  última ejecución es -1, penúltimo es -2 ...\n",
    "#time.sleep(60)\n",
    "latest_execution.wait(logs=False)\n",
    "\n",
    "print(f\"Latest execution status: {latest_execution.describe()['ProcessingJobStatus']}\")\n",
    "print(f\"Latest execution result: {latest_execution.describe()['ExitMessage']}\")\n",
    "\n",
    "latest_job = latest_execution.describe()\n",
    "if (latest_job['ProcessingJobStatus'] != 'Completed'):\n",
    "        print(\"====STOP==== \\n No completed executions to inspect further. Please wait till an execution completes or investigate previously reported failures.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_uri=latest_execution.output.destination\n",
    "print(f'Report Uri: {report_uri}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listando los informes generados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "s3uri = urlparse(report_uri)\n",
    "report_bucket = s3uri.netloc\n",
    "report_key = s3uri.path.lstrip('/')\n",
    "print(f'Report bucket: {report_bucket}')\n",
    "print(f'Report key: {report_key}')\n",
    "\n",
    "s3_client = boto3.Session().client('s3')\n",
    "result = s3_client.list_objects(Bucket=report_bucket, Prefix=report_key)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get('Contents')]\n",
    "print(\"Found Report Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informe de violaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si hay alguna violación en comparación con la línea de base, se indicará aquí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations = my_default_monitor.latest_monitoring_constraint_violations()\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "constraints_df = pd.io.json.json_normalize(violations.body_dict[\"violations\"])\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disparando la ejecución manualmente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para disparar la ejecución manualmente, primero obtenemos todas las rutas de captura de datos, estadísticas de línea base, restricciones de línea base, etc. A continuación, utilizamos la función siguiente para ejecutar el trabajo de procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile monitoringjob_utils.py\n",
    "\n",
    "import os, sys\n",
    "from urllib.parse import urlparse\n",
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "def get_model_monitor_container_uri(region):\n",
    "    container_uri_format = '{0}.dkr.ecr.{1}.amazonaws.com/sagemaker-model-monitor-analyzer'\n",
    "    \n",
    "    regions_to_accounts = {\n",
    "        'eu-north-1': '895015795356',\n",
    "        'me-south-1': '607024016150',\n",
    "        'ap-south-1': '126357580389',\n",
    "        'us-east-2': '680080141114',\n",
    "        'us-east-2': '777275614652',\n",
    "        'eu-west-1': '468650794304',\n",
    "        'eu-central-1': '048819808253',\n",
    "        'sa-east-1': '539772159869',\n",
    "        'ap-east-1': '001633400207',\n",
    "        'us-east-1': '156813124566',\n",
    "        'ap-northeast-2': '709848358524',\n",
    "        'eu-west-2': '749857270468',\n",
    "        'ap-northeast-1': '574779866223',\n",
    "        'us-west-2': '159807026194',\n",
    "        'us-west-1': '890145073186',\n",
    "        'ap-southeast-1': '245545462676',\n",
    "        'ap-southeast-2': '563025443158',\n",
    "        'ca-central-1': '536280801234'\n",
    "    }\n",
    "    \n",
    "    container_uri = container_uri_format.format(regions_to_accounts[region], region)\n",
    "    return container_uri\n",
    "\n",
    "def get_file_name(url):\n",
    "    a = urlparse(url)\n",
    "    return os.path.basename(a.path)\n",
    "\n",
    "def run_model_monitor_job_processor(\n",
    "    region, instance_type, role, data_capture_path, statistics_path,\n",
    "    constraints_path, reports_path, instance_count=1, preprocessor_path=None,\n",
    "    postprocessor_path=None, publish_cloudwatch_metrics='Disabled'\n",
    "):    \n",
    "    data_capture_sub_path = data_capture_path[data_capture_path.rfind('datacapture/') :]\n",
    "    data_capture_sub_path = data_capture_sub_path[data_capture_sub_path.find('/') + 1 :]\n",
    "    processing_output_paths = reports_path + '/' + data_capture_sub_path\n",
    "    \n",
    "    input_1 = ProcessingInput(\n",
    "        input_name='input_1',\n",
    "        source=data_capture_path,\n",
    "        destination='/opt/ml/processing/input/endpoint/' + data_capture_sub_path,\n",
    "        s3_data_type='S3Prefix',\n",
    "        s3_input_mode='File'\n",
    "    )\n",
    "\n",
    "    baseline = ProcessingInput(\n",
    "        input_name='baseline',\n",
    "        source=statistics_path,\n",
    "        destination='/opt/ml/processing/baseline/stats',\n",
    "        s3_data_type='S3Prefix',\n",
    "        s3_input_mode='File'\n",
    "    )\n",
    "\n",
    "    constraints = ProcessingInput(\n",
    "        input_name='constraints',\n",
    "        source=constraints_path,\n",
    "        destination='/opt/ml/processing/baseline/constraints',\n",
    "        s3_data_type='S3Prefix',\n",
    "        s3_input_mode='File'\n",
    "    )\n",
    "\n",
    "    outputs = ProcessingOutput(\n",
    "        output_name='result',\n",
    "        source='/opt/ml/processing/output',\n",
    "        destination=processing_output_paths,\n",
    "        s3_upload_mode='Continuous'\n",
    "    )\n",
    "\n",
    "    env = {\n",
    "        'baseline_constraints': '/opt/ml/processing/baseline/constraints/' + get_file_name(constraints_path),\n",
    "        'baseline_statistics': '/opt/ml/processing/baseline/stats/' + get_file_name(statistics_path),\n",
    "        'dataset_format': '{\"sagemakerCaptureJson\":{\"captureIndexNames\":[\"endpointInput\",\"endpointOutput\"]}}',\n",
    "        'dataset_source': '/opt/ml/processing/input/endpoint',\n",
    "        'output_path': '/opt/ml/processing/output',\n",
    "        'publish_cloudwatch_metrics': publish_cloudwatch_metrics }\n",
    "    \n",
    "    inputs=[input_1, baseline, constraints]\n",
    "    \n",
    "    if postprocessor_path:\n",
    "        env['post_analytics_processor_script'] = '/opt/ml/processing/code/postprocessing/' + get_file_name(postprocessor_path)\n",
    "        \n",
    "        post_processor_script = ProcessingInput(\n",
    "            input_name='post_processor_script',\n",
    "            source=postprocessor_path,\n",
    "            destination='/opt/ml/processing/code/postprocessing',\n",
    "            s3_data_type='S3Prefix',\n",
    "            s3_input_mode='File'\n",
    "        )\n",
    "        inputs.append(post_processor_script)\n",
    "\n",
    "    if preprocessor_path:\n",
    "        env['record_preprocessor_script'] = '/opt/ml/processing/code/preprocessing/' + get_file_name(preprocessor_path)\n",
    "\n",
    "        pre_processor_script = ProcessingInput(\n",
    "            input_name='pre_processor_script',\n",
    "            source=preprocessor_path,\n",
    "            destination='/opt/ml/processing/code/preprocessing',\n",
    "            s3_data_type='S3Prefix',\n",
    "            s3_input_mode='File'\n",
    "        )\n",
    "        inputs.append(pre_processor_script) \n",
    "    \n",
    "    processor = Processor(\n",
    "        image_uri = get_model_monitor_container_uri(region),\n",
    "        instance_count = instance_count,\n",
    "        instance_type = instance_type,\n",
    "        role=role,\n",
    "        env = env\n",
    "    )\n",
    "\n",
    "    return processor.run(inputs=inputs, outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = s3_client.list_objects(Bucket=bucket, Prefix='tf-2-workflow/monitoring/datacapture/')\n",
    "capture_files = [f's3://{bucket}/{capture_file.get(\"Key\")}' for capture_file in result.get('Contents')]\n",
    "\n",
    "print(\"Capture Files: \")\n",
    "print(\"\\n\".join(capture_files))\n",
    "\n",
    "data_capture_path = capture_files[len(capture_files) - 1][: capture_files[len(capture_files) - 1].rfind('/')]\n",
    "statistics_path = baseline_results_uri + '/statistics.json'\n",
    "constraints_path = baseline_results_uri + '/constraints.json'\n",
    "\n",
    "print(data_capture_path)\n",
    "print(preprocessor_s3_dest)\n",
    "print(statistics_path)\n",
    "print(constraints_path)\n",
    "print(s3_report_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monitoringjob_utils import run_model_monitor_job_processor\n",
    "\n",
    "processor = run_model_monitor_job_processor(\n",
    "    region, 'ml.m5.xlarge', \n",
    "    role, \n",
    "    data_capture_path, \n",
    "    statistics_path, \n",
    "    constraints_path, \n",
    "    s3_report_path,\n",
    "    preprocessor_path=preprocessor_s3_dest\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspección de la ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def get_latest_model_monitor_processing_job_name(base_job_name):\n",
    "    client = boto3.client('sagemaker')\n",
    "    response = client.list_processing_jobs(\n",
    "        NameContains=base_job_name,\n",
    "        SortBy='CreationTime', \n",
    "        SortOrder='Descending',\n",
    "        StatusEquals='Completed'\n",
    "    )\n",
    "    \n",
    "    if len(response['ProcessingJobSummaries']) > 0 :\n",
    "        return response['ProcessingJobSummaries'][0]['ProcessingJobName']\n",
    "    else:\n",
    "        raise Exception('Processing job not found.')\n",
    "\n",
    "def get_model_monitor_processing_job_s3_report(job_name):\n",
    "    client = boto3.client('sagemaker')\n",
    "    response = client.describe_processing_job(ProcessingJobName=job_name)\n",
    "    s3_report_path = response['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']\n",
    "    return s3_report_path\n",
    "\n",
    "latest_model_monitor_processing_job_name = get_latest_model_monitor_processing_job_name('sagemaker-model-monitor-analyzer')\n",
    "print(latest_model_monitor_processing_job_name)\n",
    "report_path = get_model_monitor_processing_job_s3_report(latest_model_monitor_processing_job_name)\n",
    "print(report_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df = pd.read_json('{}/constraint_violations.json'.format(report_path))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Borrar los recursos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes mantener el endpoint en funcionamiento para seguir capturando datos. Si no tienes previsto recopilar más datos ni seguir utilizando este endpoint, debe eliminarlo para evitar incurrir en cargos adicionales. Ten en cuenta que la eliminación del endpoint no elimina los datos que se capturaron durante las invocaciones del modelo. Esos datos persisten en Amazon S3 hasta los elimines.\n",
    "\n",
    "Pero antes de eso, debes eliminar primero el schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_default_monitor.delete_monitoring_schedule()\n",
    "time.sleep(120) # Esperar hasta que se borre definitivamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3.9.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sagemaker "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformar los datos y entrenar un modelo dentro de un notebook Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El kernel utilizado en este notebook es `Python 3 (TensorFlow 2.6 Python 3.8 CPU Optimized`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "En este notebook demostraremos un viaje hacia el aprendizaje automático nativo en la nube, empezando por un enfoque más tradicional de desarrollo de modelos y entrenamiento directamente en notebooks Jupyter, pasando por transformaciones de datos gestionadas de forma remota y entrenamiento con Amazon SageMaker, hasta pipelines totalmente automatizados con [SageMaker Pipelines](https://aws.amazon.com/sagemaker/pipelines/).\n",
    "\n",
    "En este primer cuaderno predeciremos el precio de la vivienda basándonos en el conocido [Boston Housing dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) con un modelo de regresión simple en Tensorflow 2. Este Dataset público contiene 13 features relativas al parque de viviendas de las ciudades del área de Boston. Las features incluyen el número medio de habitaciones, la accesibilidad a las carreteras radiales, la adyacencia a un río importante, etc.  \n",
    "\n",
    "Para empezar, crearemos directorios para los datos de entrenamiento y de prueba.  También configuraremos una sesión de SageMaker para realizar varias operaciones, y especificaremos un cubo de Amazon S3 para mantener los datos de entrada y salida.  El cubo por defecto utilizado aquí es creado por SageMaker si no existe ya, y nombrado de acuerdo con el ID de la cuenta de AWS y la región de AWS.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "train_dir = os.path.join(os.getcwd(), 'data/train')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "test_dir = os.path.join(os.getcwd(), 'data/test')\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "raw_dir = os.path.join(os.getcwd(), 'data/raw')\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "\n",
    "batch_dir = os.path.join(os.getcwd(), 'data/batch')\n",
    "os.makedirs(batch_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "According to The [State of Data Science 2020](https://www.anaconda.com/state-of-data-science-2020) survey, data management, exploratory data analysis (EDA), feature selection, and feature engineering accounts for more than 66% of a data scientist’s time.\n",
    "\n",
    "Exploratory Data Analysis is an approach in analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods.\n",
    "EDA assists Data science professionals in various ways:-\n",
    "\n",
    "- Getting a better understanding of data.\n",
    "- Identifying various data patterns.\n",
    "- Getting a better understanding of the problem statement.\n",
    "\n",
    "Numerical EDA gives you some very important information, such as the names and data types of the columns, and the dimensions of the DataFrame. \n",
    "Visual EDA on the other hand will give you insight into features and target relationship and distribution.\n",
    "\n",
    "First we'll load the Boston Housing dataset and explore the data.\n",
    "\n",
    "Según la encuesta [State of Data Science 2020](https://www.anaconda.com/state-of-data-science-2020), la gestión de datos, el análisis exploratorio de datos (EDA), la selección de features y la ingeniería de features representan más del 66% del tiempo de un científico de datos.\n",
    "\n",
    "El análisis exploratorio de datos es un enfoque para analizar Datasets con el fin de resumir sus principales características, a menudo utilizando gráficos estadísticos y otros métodos de visualización de datos.\n",
    "El EDA ayuda a los profesionales de la ciencia de los datos de varias maneras\n",
    "\n",
    "- Obtener una mejor comprensión de los datos.\n",
    "- Identificar patrones en los datos.\n",
    "- Obtener una mejor comprensión del planteamiento del problema.\n",
    "\n",
    "El EDA numérico da información muy importante, como los nombres y tipos de datos de las columnas, y las dimensiones del DataFrame.\n",
    "El EDA visual, por otro lado, da una visión de la relación entre las features y el target y de la distribución de las muestras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero cargaremos el Dataset de viviendas de Boston y exploraremos los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.datasets import boston_housing\n",
    "\n",
    "# El Dataset ya tiene un split hecho al cargarse\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['CRIM',\n",
    "         'ZN',\n",
    "         'INDUS',\n",
    "         'CHAS',\n",
    "         'NOX',\n",
    "         'RM',\n",
    "         'AGE',\n",
    "         'DIS',\n",
    "         'RAD',\n",
    "         'TAX',\n",
    "         'PTRATIO',\n",
    "         'B',\n",
    "         'LSTAT']\n",
    "\n",
    "df = pd.DataFrame(x_train, columns=columns)\n",
    "df[\"MEDV\"] = y_train\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA numérico\n",
    "\n",
    "Comprobamos qué tamaño tiene el Dataset, cuántas features tiene y de qué tipo, y cuál es el target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay 14 atributos en cada caso del Dataset:\n",
    "\n",
    "1. CRIM - tasa de criminalidad per cápita por ciudad.\n",
    "2. ZN - proporción de terrenos residenciales agrupados en solares de más de 25.000 pies cuadrados.\n",
    "3. INDUS - proporción de acres comerciales no minoristas por ciudad.\n",
    "4. CHAS - Variable ficticia del río Charles (1 si el tramo limita con el río; 0 en caso contrario).\n",
    "5. NOX - concentración de óxidos nítricos (partes por 10 millones).\n",
    "6. RM - número medio de habitaciones por vivienda.\n",
    "7. EDAD - proporción de unidades habitadas por sus propietarios construidas antes de 1940.\n",
    "8. DIS - distancias ponderadas a cinco centros de empleo de Boston.\n",
    "9. RAD - índice de accesibilidad a las autopistas radiales.\n",
    "10. TAX - tasa de impuesto sobre el valor total de la propiedad por cada 10.000 dólares.\n",
    "11. PTRATIO - ratio alumno-profesor por ciudad.\n",
    "12. B - 1000(Bk - 0.63)^2 donde Bk es la proporción de gente de color por ciudad.\n",
    "13. LSTAT - % de estatus inferior de la población.\n",
    "14. MEDV - Valor medio de las viviendas habitadas por sus propietarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a resumir los datos para ver la distribución de los mismos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA visual\n",
    "\n",
    "Intentemos encontrar el nivel de delincuencia en relación con la condición de ciudadano y el valor medio de las viviendas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['medv_bins'] = pd.cut(df.MEDV, bins=5, include_lowest=True)\n",
    "df.medv_bins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lstat_bins'] = pd.cut(df.LSTAT, bins=[0, 7, 17, 38], labels=['richest', 'ordinary', 'poorest'], include_lowest=True)\n",
    "df.lstat_bins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax_viol, ax_box) = plt.subplots(2, sharex=True)\n",
    "\n",
    "sns.boxplot(x='medv_bins', y='CRIM', data=df, ax=ax_viol)\n",
    "sns.swarmplot(x='medv_bins', y='CRIM', hue='lstat_bins', data=df, ax=ax_box)\n",
    "\n",
    "ax_viol.set(xlabel='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que a medida que el estatus de los ciudadanos es más pobre (LSTAT) y el valor medio de las casas es más bajo (MEDV), el nivel de delincuencia está creciendo (CRIM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Dataset transformation <a class=\"anchor\" id=\"SageMakerProcessing\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, transformaremos el Dataset. En un flujo de trabajo típico de SageMaker, los notebooks sólo se utilizan para la creación de prototipos y pueden ejecutarse en instancias relativamente baratas y menos potentes, mientras que las tareas de procesamiento, formación y alojamiento de modelos se ejecutan en instancias independientes y más potentes gestionadas por SageMaker.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora guardaremos los datos de features en bruto, y también guardaremos las etiquetas para el entrenamiento y las pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.save(os.path.join(raw_dir, 'x_train.npy'), x_train)\n",
    "np.save(os.path.join(raw_dir, 'x_test.npy'), x_test)\n",
    "np.save(os.path.join(raw_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(raw_dir, 'y_test.npy'), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A continuación, ejecutaremos el preprocesamiento de datos como se muestra a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = np.load(os.path.join(raw_dir, 'x_train.npy'))\n",
    "scaler.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "input_files = glob.glob('{}/raw/*.npy'.format(data_dir))\n",
    "print('\\nINPUT FILE LIST: \\n{}\\n'.format(input_files))\n",
    "for file in input_files:\n",
    "    raw = np.load(file)\n",
    "    # solo transformamos las columnas de features\n",
    "    if 'y_' not in file:\n",
    "        transformed = scaler.transform(raw)\n",
    "    if 'train' in file:\n",
    "        if 'y_' in file:\n",
    "            output_path = os.path.join(train_dir, 'y_train.npy')\n",
    "            np.save(output_path, raw)\n",
    "            print('SAVED LABEL TRAINING DATA FILE\\n')\n",
    "        else:\n",
    "            output_path = os.path.join(train_dir, 'x_train.npy')\n",
    "            np.save(output_path, transformed)\n",
    "            print('SAVED TRANSFORMED TRAINING DATA FILE\\n')\n",
    "    else:\n",
    "        if 'y_' in file:\n",
    "            output_path = os.path.join(test_dir, 'y_test.npy')\n",
    "            np.save(output_path, raw)\n",
    "            print('SAVED LABEL TEST DATA FILE\\n')\n",
    "        else:\n",
    "            output_path = os.path.join(test_dir, 'x_test.npy')\n",
    "            np.save(output_path, transformed)\n",
    "            print('SAVED TRANSFORMED TEST DATA FILE\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#  Training <a class=\"anchor\" id=\"SageMakerHostedTraining\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos preparado un conjunto de datos, podemos pasar al entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "def get_train_data(train_dir):\n",
    "    x_train = np.load(os.path.join(train_dir, 'x_train.npy'))\n",
    "    y_train = np.load(os.path.join(train_dir, 'y_train.npy'))\n",
    "    print('x train', x_train.shape,'y train', y_train.shape)\n",
    "\n",
    "    return x_train, y_train\n",
    "\n",
    "\n",
    "def get_test_data(test_dir):\n",
    "    x_test = np.load(os.path.join(test_dir, 'x_test.npy'))\n",
    "    y_test = np.load(os.path.join(test_dir, 'y_test.npy'))\n",
    "    print('x test', x_test.shape,'y test', y_test.shape)\n",
    "\n",
    "    return x_test, y_test\n",
    "\n",
    "def get_model():\n",
    "    inputs = tf.keras.Input(shape=(13,))\n",
    "    hidden_1 = tf.keras.layers.Dense(13, activation='tanh')(inputs)\n",
    "    hidden_2 = tf.keras.layers.Dense(6, activation='sigmoid')(hidden_1)\n",
    "    outputs = tf.keras.layers.Dense(1)(hidden_2)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "x_train, y_train = get_train_data(train_dir)\n",
    "x_test, y_test = get_test_data(test_dir)\n",
    "\n",
    "device = '/cpu:0'\n",
    "print(device)\n",
    "batch_size = 128\n",
    "epochs = 80\n",
    "learning_rate = 0.01\n",
    "print('batch_size = {}, epochs = {}, learning rate = {}'.format(batch_size, epochs, learning_rate))\n",
    "\n",
    "with tf.device(device):\n",
    "    model = get_model()\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=(x_test, y_test))\n",
    "\n",
    "    # evaluar en el Dataset\n",
    "    scores = model.evaluate(x_test, y_test, batch_size, verbose=2)\n",
    "    print(\"\\nTest MSE :\", scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "El archivo descomprimido debe incluir los activos requeridos por TensorFlow Serving para cargar el modelo y servirlo, incluyendo un archivo .pb:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.save('model' + '/1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Puntuación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model('model/1')\n",
    "\n",
    "x_test = np.load(os.path.join(test_dir, 'x_test.npy'))\n",
    "y_test = np.load(os.path.join(test_dir, 'y_test.npy'))\n",
    "scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"\\nTest MSE :\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.6 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.6-cpu-py38-ubuntu20.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
